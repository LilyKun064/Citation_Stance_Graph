{
  "config": {
    "id": "36a3b0b5-bad0-4a04-b79b-441c7cef77db",
    "label": "BetterBibTeX JSON",
    "preferences": {
      "ascii": "",
      "asciiBibLaTeX": false,
      "asciiBibTeX": true,
      "autoAbbrev": false,
      "autoExport": "immediate",
      "autoExportDelay": 5,
      "autoExportIdleWait": 10,
      "autoExportPathReplaceDiacritics": false,
      "autoExportPathReplaceDirSep": "-",
      "autoExportPathReplaceSpace": " ",
      "automaticTags": true,
      "autoPinDelay": 0,
      "auxImport": false,
      "baseAttachmentPath": "",
      "biblatexExtendedDateFormat": true,
      "biblatexExtendedNameFormat": true,
      "biblatexExtractEprint": true,
      "biblatexUsePrefix": true,
      "bibtexEditionOrdinal": false,
      "bibtexParticleNoOp": false,
      "bibtexURL": "off",
      "cache": true,
      "cacheDelete": false,
      "charmap": "",
      "chinese": false,
      "chineseSplitName": true,
      "citeCommand": "cite",
      "citekeyCaseInsensitive": true,
      "citekeyFold": true,
      "citekeyFormat": "auth.lower + shorttitle(3, 3) + year",
      "citekeySearch": true,
      "citekeyUnsafeChars": "\\\"#%'(),={}~",
      "csquotes": "",
      "DOIandURL": "both",
      "exportBibTeXStrings": "off",
      "exportBraceProtection": true,
      "exportSort": "citekey",
      "exportTitleCase": true,
      "extraMergeCitekeys": false,
      "extraMergeCSL": false,
      "extraMergeTeX": false,
      "git": "config",
      "import": true,
      "importBibTeXStrings": true,
      "importCaseProtection": "as-needed",
      "importCitationKey": true,
      "importDetectURLs": true,
      "importExtra": true,
      "importJabRefAbbreviations": true,
      "importJabRefStrings": true,
      "importNoteToExtra": "",
      "importPlaceEvent": "inproceedings,conference,presentation,talk",
      "importSentenceCase": "on+guess",
      "importSentenceCaseQuoted": true,
      "importUnknownTexCommand": "ignore",
      "itemObserverDelay": 5,
      "jabrefFormat": 0,
      "japanese": false,
      "keyConflictPolicy": "keep",
      "keyScope": "library",
      "language": "langid",
      "mapMath": "",
      "mapText": "",
      "packages": "",
      "parseParticles": true,
      "patchDates": "dateadded=dateAdded, date-added=dateAdded, datemodified=dateModified, date-modified=dateModified",
      "postscript": "",
      "postscriptOverride": "",
      "preferencesOverride": "",
      "qualityReport": false,
      "quickCopyEta": "",
      "quickCopyMode": "latex",
      "quickCopyOrgMode": "zotero",
      "quickCopyPandocBrackets": false,
      "quickCopySelectLink": "zotero",
      "rawImports": false,
      "rawLaTag": "#LaTeX",
      "relativeFilePaths": false,
      "separatorList": "and",
      "separatorNames": "and",
      "skipFields": "",
      "skipWords": "a,ab,aboard,about,above,across,after,against,al,along,amid,among,an,and,anti,around,as,at,before,behind,below,beneath,beside,besides,between,beyond,but,by,d,da,das,de,del,dell,dello,dei,degli,della,dell,delle,dem,den,der,des,despite,die,do,down,du,during,ein,eine,einem,einen,einer,eines,el,en,et,except,for,from,gli,i,il,in,inside,into,is,l,la,las,le,les,like,lo,los,near,nor,of,off,on,onto,or,over,past,per,plus,round,save,since,so,some,sur,than,the,through,to,toward,towards,un,una,unas,under,underneath,une,unlike,uno,unos,until,up,upon,versus,via,von,while,with,within,without,yet,zu,zum",
      "startupProgress": "popup",
      "strings": "",
      "stringsOverride": "",
      "verbatimFields": "url,doi,file,pdf,ids,eprint,/^verb[a-z]$/,groups,/^citeulike-linkout-[0-9]+$/, /^bdsk-url-[0-9]+$/, keywords",
      "warnBulkModify": 10,
      "warnTitleCased": false
    },
    "options": {
      "Items": true,
      "Normalize": false,
      "Preferences": true,
      "exportFileData": false,
      "exportNotes": false,
      "keepUpdated": false,
      "worker": true,
      "exportDir": "E:\\LLM_Gender_Bias\\manuscript",
      "exportPath": "E:\\LLM_Gender_Bias\\manuscript\\LLM_Gender_Bias.json"
    }
  },
  "version": {
    "zotero": "7.0.30",
    "bbt": "7.0.68"
  },
  "collections": {},
  "items": [
    {
      "key": "FK9PL2XP",
      "version": 0,
      "itemType": "note",
      "note": "",
      "tags": [],
      "collections": [
        "WEQV9LR3"
      ],
      "relations": {},
      "dateAdded": "2025-11-28T04:07:09Z",
      "dateModified": "2025-11-28T04:07:09Z",
      "uri": "http://zotero.org/users/12031879/items/FK9PL2XP",
      "itemID": 2274,
      "itemKey": "FK9PL2XP",
      "libraryID": 1,
      "select": "zotero://select/library/items/FK9PL2XP"
    },
    {
      "key": "MDDHKDP2",
      "version": 5046,
      "itemType": "journalArticle",
      "title": "Gender bias in natural language processing and computer vision: a comparative survey",
      "abstractNote": "Taking an interdisciplinary approach to surveying issues around gender bias in textual and visual AI, we present literature on gender bias detection and mitigation in NLP, CV, as well as combined visual-linguistic models. We identify conceptual parallels between these strands of research as well as how methodologies were adapted cross-disciplinary from NLP to CV. We also find that there is a growing awareness for theoretical frameworks from the social sciences around gender in NLP that could be beneficial for aligning bias analytics in CV with human values and conceptualising gender beyond the binary categories of male/female.",
      "date": "2025-02",
      "url": "https://doi.org/10.1145/3700438",
      "extra": "Citation Key: 10.1145/3700438\nNumber of pages: 36\nPlace: New York, NY, USA\nPublisher: Association for Computing Machinery\ntex.articleno: 139\ntex.issue_date: June 2025",
      "volume": "57",
      "publicationTitle": "Acm Computing Surveys",
      "DOI": "10.1145/3700438",
      "issue": "6",
      "journalAbbreviation": "ACM Comput. Surv.",
      "ISSN": "0360-0300",
      "creators": [
        {
          "firstName": "Marion",
          "lastName": "Bartl",
          "creatorType": "author"
        },
        {
          "firstName": "Abhishek",
          "lastName": "Mandal",
          "creatorType": "author"
        },
        {
          "firstName": "Susan",
          "lastName": "Leavy",
          "creatorType": "author"
        },
        {
          "firstName": "Suzanne",
          "lastName": "Little",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "computer vision"
        },
        {
          "tag": "ethical AI"
        },
        {
          "tag": "natural language processing"
        },
        {
          "tag": "Trustworthy AI"
        }
      ],
      "relations": {},
      "dateAdded": "2025-11-23T04:01:58Z",
      "dateModified": "2025-11-23T04:01:58Z",
      "uri": "http://zotero.org/users/12031879/items/MDDHKDP2",
      "itemID": 2188,
      "attachments": [],
      "notes": [],
      "citationKey": "10.1145/3700438",
      "itemKey": "MDDHKDP2",
      "libraryID": 1,
      "select": "zotero://select/library/items/MDDHKDP2"
    },
    {
      "key": "MSKCXELC",
      "version": 5151,
      "itemType": "conferencePaper",
      "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
      "abstractNote": "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",
      "date": "2016",
      "extra": "Citation Key: 10.5555/3157382.3157584\nNumber of pages: 9\ntex.address: Red Hook, NY, USA",
      "place": "Barcelona, Spain",
      "publisher": "Curran Associates Inc.",
      "ISBN": "978-1-5108-3881-9",
      "pages": "4356–4364",
      "series": "NIPS'16",
      "proceedingsTitle": "Proceedings of the 30th international conference on neural information processing systems",
      "creators": [
        {
          "firstName": "Tolga",
          "lastName": "Bolukbasi",
          "creatorType": "author"
        },
        {
          "firstName": "Kai-Wei",
          "lastName": "Chang",
          "creatorType": "author"
        },
        {
          "firstName": "James",
          "lastName": "Zou",
          "creatorType": "author"
        },
        {
          "firstName": "Venkatesh",
          "lastName": "Saligrama",
          "creatorType": "author"
        },
        {
          "firstName": "Adam",
          "lastName": "Kalai",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-25T01:15:55Z",
      "dateModified": "2025-11-25T01:15:55Z",
      "uri": "http://zotero.org/users/12031879/items/MSKCXELC",
      "itemID": 2240,
      "attachments": [],
      "notes": [],
      "citationKey": "10.5555/3157382.3157584",
      "itemKey": "MSKCXELC",
      "libraryID": 1,
      "select": "zotero://select/library/items/MSKCXELC"
    },
    {
      "key": "2QG62896",
      "version": 5110,
      "itemType": "conferencePaper",
      "title": "Spoken stereoset: on evaluating social bias toward speaker in speech large language models",
      "date": "2024",
      "extra": "Citation Key: 10832259",
      "pages": "871-878",
      "proceedingsTitle": "2024 IEEE spoken language technology workshop (SLT)",
      "DOI": "10.1109/SLT61566.2024.10832259",
      "creators": [
        {
          "firstName": "Yi-Cheng",
          "lastName": "Lin",
          "creatorType": "author"
        },
        {
          "firstName": "Wei-Chih",
          "lastName": "Chen",
          "creatorType": "author"
        },
        {
          "firstName": "Hung-Yi",
          "lastName": "Lee",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Conferences"
        },
        {
          "tag": "Data models"
        },
        {
          "tag": "Large language models"
        },
        {
          "tag": "LLM"
        },
        {
          "tag": "Prevention and mitigation"
        },
        {
          "tag": "social bias"
        },
        {
          "tag": "speech large language model"
        },
        {
          "tag": "Testing"
        },
        {
          "tag": "Training"
        },
        {
          "tag": "Training data"
        }
      ],
      "relations": {},
      "dateAdded": "2025-11-23T04:20:47Z",
      "dateModified": "2025-11-23T04:20:47Z",
      "uri": "http://zotero.org/users/12031879/items/2QG62896",
      "itemID": 2231,
      "attachments": [],
      "notes": [],
      "citationKey": "10832259",
      "itemKey": "2QG62896",
      "libraryID": 1,
      "select": "zotero://select/library/items/2QG62896"
    },
    {
      "key": "7HJHQF38",
      "version": 5151,
      "itemType": "journalArticle",
      "title": "Measuring gender and racial biases in large language models: Intersectional evidence from automated resume evaluation",
      "abstractNote": "Abstract\n            In traditional decision-making processes, social biases of human decision makers can lead to unequal economic outcomes for underrepresented social groups, such as women and racial/ethnic minorities (1–4). Recently, the growing popularity of large language model (LLM)-based AI signals a potential shift from human to AI-based decision-making. How would this transition affect the distributional outcomes across social groups? Here, we investigate the gender and racial biases of a number of commonly used LLMs, including OpenAI's GPT-3.5 Turbo and GPT-4o, Google's Gemini 1.5 Flash, Anthropic AI's Claude 3.5 Sonnet, and Meta's Llama 3-70b, in a high-stakes decision-making setting of assessing entry-level job candidates from diverse social groups. Instructing the models to score ∼361,000 resumes with randomized social identities, we find that the LLMs award higher assessment scores for female candidates with similar work experience, education, and skills, but lower scores for black male candidates with comparable qualifications. These biases may result in ∼1–3 percentage-point differences in hiring probabilities for otherwise similar candidates at a certain threshold and are consistent across various job positions and subsamples. Meanwhile, many models are biased against black male candidates. Our results indicate that LLM-based AI systems demonstrate significant biases, varying in terms of the directions and magnitudes across different social groups. Further research is needed to comprehend the root causes of these outcomes and develop strategies to minimize the remaining biases in AI systems. As AI-based decision-making tools are increasingly employed across diverse domains, our findings underscore the necessity of understanding and addressing the potential unequal outcomes to ensure equitable outcomes across social groups.",
      "date": "2025-02-27",
      "language": "en",
      "shortTitle": "Measuring gender and racial biases in large language models",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://academic.oup.com/pnasnexus/article/doi/10.1093/pnasnexus/pgaf089/8071848",
      "accessDate": "2025-11-25T01:17:51Z",
      "rights": "https://creativecommons.org/licenses/by-nc/4.0/",
      "volume": "4",
      "pages": "pgaf089",
      "publicationTitle": "PNAS Nexus",
      "DOI": "10.1093/pnasnexus/pgaf089",
      "issue": "3",
      "ISSN": "2752-6542",
      "creators": [
        {
          "firstName": "Jiafu",
          "lastName": "An",
          "creatorType": "author"
        },
        {
          "firstName": "Difang",
          "lastName": "Huang",
          "creatorType": "author"
        },
        {
          "firstName": "Chen",
          "lastName": "Lin",
          "creatorType": "author"
        },
        {
          "firstName": "Mingzhu",
          "lastName": "Tai",
          "creatorType": "author"
        },
        {
          "firstName": "Jay",
          "lastName": "Van Bavel",
          "creatorType": "editor"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-25T01:17:51Z",
      "dateModified": "2025-11-25T01:17:51Z",
      "uri": "http://zotero.org/users/12031879/items/7HJHQF38",
      "itemID": 2249,
      "attachments": [],
      "notes": [],
      "citationKey": "anMeasuringGenderRacial2025",
      "itemKey": "7HJHQF38",
      "libraryID": 1,
      "select": "zotero://select/library/items/7HJHQF38"
    },
    {
      "key": "MYKWKE3N",
      "version": 5024,
      "itemType": "journalArticle",
      "title": "Intergenerational evolution of gender bias in spain: Analysis of values surveys",
      "date": "2025",
      "url": "https://www.cogitatiopress.com/socialinclusion/article/view/9288",
      "extra": "Citation Key: antolinez-merchan_intergenerational_2025",
      "volume": "13",
      "pages": "9288",
      "publicationTitle": "Social Inclusion",
      "DOI": "10.17645/si.9288",
      "ISSN": "2183-2803",
      "creators": [
        {
          "firstName": "Pilar",
          "lastName": "Antolínez-Merchán",
          "creatorType": "author"
        },
        {
          "firstName": "Ángel",
          "lastName": "Rivero Recuenco",
          "creatorType": "author"
        },
        {
          "firstName": "Elvira Carmen",
          "lastName": "Cabrera-Rodríguez",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T03:44:20Z",
      "dateModified": "2025-11-23T03:44:20Z",
      "uri": "http://zotero.org/users/12031879/items/MYKWKE3N",
      "itemID": 2166,
      "attachments": [],
      "notes": [],
      "citationKey": "antolinez-merchan_intergenerational_2025",
      "itemKey": "MYKWKE3N",
      "libraryID": 1,
      "select": "zotero://select/library/items/MYKWKE3N"
    },
    {
      "key": "7BREL6J9",
      "version": 5025,
      "itemType": "journalArticle",
      "title": "Gender stereotypes about mathematics and science and self-perceptions of ability in late childhood and early adolescence",
      "date": "2008",
      "url": "https://muse.jhu.edu/article/239783",
      "extra": "Citation Key: beth_kurtz-costes_gender_2008",
      "volume": "54",
      "pages": "386–409",
      "publicationTitle": "Merrill-Palmer Quarterly",
      "DOI": "10.1353/mpq.0.0001",
      "issue": "3",
      "ISSN": "1535-0266",
      "creators": [
        {
          "firstName": "Beth",
          "lastName": "Kurtz-Costes",
          "creatorType": "author"
        },
        {
          "firstName": "Stephanie J.",
          "lastName": "Rowley",
          "creatorType": "author"
        },
        {
          "firstName": "April",
          "lastName": "Harris-Britt",
          "creatorType": "author"
        },
        {
          "firstName": "Taniesha A.",
          "lastName": "Woods",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T03:44:20Z",
      "dateModified": "2025-11-23T03:44:20Z",
      "uri": "http://zotero.org/users/12031879/items/7BREL6J9",
      "itemID": 2174,
      "attachments": [],
      "notes": [],
      "citationKey": "beth_kurtz-costes_gender_2008",
      "itemKey": "7BREL6J9",
      "libraryID": 1,
      "select": "zotero://select/library/items/7BREL6J9"
    },
    {
      "key": "2GMIDUT6",
      "version": 5024,
      "itemType": "document",
      "title": "Approach intelligent writing assistants usability with seven stages of action",
      "date": "2023",
      "url": "https://arxiv.org/abs/2304.02822",
      "extra": "Citation Key: bhat_approach_2023\nDOI: 10.48550/arXiv.2304.02822\ntex.howpublished: arXiv preprint",
      "creators": [
        {
          "firstName": "Avinash",
          "lastName": "Bhat",
          "creatorType": "author"
        },
        {
          "firstName": "Disha",
          "lastName": "Shrivastava",
          "creatorType": "author"
        },
        {
          "firstName": "Jin L. C.",
          "lastName": "Guo",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T03:44:20Z",
      "dateModified": "2025-11-23T03:44:20Z",
      "uri": "http://zotero.org/users/12031879/items/2GMIDUT6",
      "itemID": 2169,
      "attachments": [],
      "notes": [],
      "citationKey": "bhat_approach_2023",
      "itemKey": "2GMIDUT6",
      "libraryID": 1,
      "select": "zotero://select/library/items/2GMIDUT6"
    },
    {
      "key": "3G8R7A3R",
      "version": 5024,
      "itemType": "journalArticle",
      "title": "Quantifying cultural change: Gender bias in music",
      "date": "2023",
      "extra": "Citation Key: boghrati_quantifying_2023",
      "volume": "152",
      "pages": "2591–2602",
      "publicationTitle": "Journal of Experimental Psychology: General",
      "DOI": "10.1037/xge0001412",
      "issue": "9",
      "ISSN": "1939-2222",
      "creators": [
        {
          "firstName": "Reihane",
          "lastName": "Boghrati",
          "creatorType": "author"
        },
        {
          "firstName": "Jonah",
          "lastName": "Berger",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T03:44:20Z",
      "dateModified": "2025-11-23T03:44:20Z",
      "uri": "http://zotero.org/users/12031879/items/3G8R7A3R",
      "itemID": 2165,
      "attachments": [],
      "notes": [],
      "citationKey": "boghrati_quantifying_2023",
      "itemKey": "3G8R7A3R",
      "libraryID": 1,
      "select": "zotero://select/library/items/3G8R7A3R"
    },
    {
      "key": "HFZQGBWY",
      "version": 5038,
      "itemType": "preprint",
      "title": "Identifying and Reducing Gender Bias in Word-Level Language Models",
      "abstractNote": "Many text corpora exhibit socially problematic biases, which can be propagated or amplified in the models trained on such data. For example, doctor cooccurs more frequently with male pronouns than female pronouns. In this study we (i) propose a metric to measure gender bias; (ii) measure bias in a text corpus and the text generated from a recurrent neural network language model trained on the text corpus; (iii) propose a regularization loss term for the language model that minimizes the projection of encoder-trained embeddings onto an embedding subspace that encodes gender; (iv) finally, evaluate efficacy of our proposed method on reducing gender bias. We find this regularization method to be effective in reducing gender bias up to an optimal weight assigned to the loss term, beyond which the model becomes unstable as the perplexity increases. We replicate this study on three training corpora---Penn Treebank, WikiText-2, and CNN/Daily Mail---resulting in similar conclusions.",
      "date": "2019-04-05",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1904.03035",
      "accessDate": "2025-11-23T03:50:05Z",
      "extra": "arXiv:1904.03035 [cs]",
      "DOI": "10.48550/arXiv.1904.03035",
      "repository": "arXiv",
      "archiveID": "arXiv:1904.03035",
      "creators": [
        {
          "firstName": "Shikha",
          "lastName": "Bordia",
          "creatorType": "author"
        },
        {
          "firstName": "Samuel R.",
          "lastName": "Bowman",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Computation and Language",
          "type": 1
        }
      ],
      "relations": {},
      "dateAdded": "2025-11-23T03:50:05Z",
      "dateModified": "2025-11-23T03:50:05Z",
      "uri": "http://zotero.org/users/12031879/items/HFZQGBWY",
      "itemID": 2180,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Preprint PDF",
          "url": "http://arxiv.org/pdf/1904.03035v1",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-23T03:50:05Z",
          "dateModified": "2025-11-23T03:50:05Z",
          "uri": "http://zotero.org/users/12031879/items/LGMI9PWW",
          "path": "C:\\Users\\lilyc\\Zotero\\storage\\LGMI9PWW\\Bordia and Bowman - 2019 - Identifying and Reducing Gender Bias in Word-Level Language Models.pdf",
          "select": "zotero://select/library/items/LGMI9PWW"
        },
        {
          "itemType": "attachment",
          "title": "Snapshot",
          "url": "https://arxiv.org/abs/1904.03035",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-23T03:50:10Z",
          "dateModified": "2025-11-23T03:50:10Z",
          "uri": "http://zotero.org/users/12031879/items/ASRDPJ6Y",
          "path": "C:\\Users\\lilyc\\Zotero\\storage\\ASRDPJ6Y\\1904.html",
          "select": "zotero://select/library/items/ASRDPJ6Y"
        }
      ],
      "notes": [
        {
          "key": "TXP5FW6J",
          "version": 5185,
          "itemType": "note",
          "parentItem": "HFZQGBWY",
          "note": "<div data-schema-version=\"9\"><h1>Trade-off Between Reducing Bias and Model Performance</h1>\n<p>This paper investigates how gender bias appears in word-level language models and proposes a way to measure and reduce that bias while tracking the impact on model quality.</p>\n<h2>Background and Motivation</h2>\n<p>Language models learn from large text collections (corpora) and are used in applications like predictive keyboards and text generation. If the training data are biased—for example, if “doctor” appears more often near “he” than “she”—the model can learn and reproduce those patterns, reinforcing stereotypes when it generates text. The authors focus specifically on gender bias in word-level language models and ask:</p>\n<ol>\n<li>\nHow can we <strong>measure</strong> gender bias in both training data and model outputs?\n</li>\n<li>\nDo language models <strong>amplify</strong> the bias in their training corpora?\n</li>\n<li>\nCan we <strong>reduce</strong> bias learned by the model without severely damaging its performance? [1][3]\n</li>\n</ol>\n<h2>Core Concepts</h2>\n<h3>Gendered and Neutral Words</h3>\n<p>The authors define:</p>\n<ul>\n<li>\n<strong>Gendered words</strong>: explicitly gender-marked terms like “he,” “she,” “man,” “woman,” “daughter,” “husband,” etc., tailored per corpus depending on which words actually appear [9].\n</li>\n<li>\n<strong>Gender-neutral words</strong>: all other content words, excluding stop words and gendered words, whose associations with gender (e.g., “nurse,” “leader,” “fragile”) are of interest.\n</li>\n</ul>\n<h3>Gender Bias via Co-occurrence</h3>\n<p>They operationalize gender bias as how often a given word appears near male vs. female gendered words in text. For any word <em>w</em> and a gender category <em>g</em> (male or female), they estimate:</p>\n<ul>\n<li>\n( P(w | g) ): probability that <em>w</em> appears in the context of a gendered word of type <em>g</em>.<br>They then define a <strong>bias score</strong> for each word:\n</li>\n<li>\n( \\text{bias}_{train}(w) = \\log \\frac{P(w | \\text{female})}{P(w | \\text{male})} ) [4]<br>Interpretation:\n</li>\n<li>\nPositive score: <em>w</em> appears more with female words (female-associated).\n</li>\n<li>\nNegative score: <em>w</em> appears more with male words (male-associated).\n</li>\n<li>\nZero: balanced between genders.<br>They compute this for:\n</li>\n</ul>\n<ol>\n<li>\nThe <strong>training corpus</strong>, to characterize inherent bias.\n</li>\n<li>\n<strong>Text generated</strong> by the trained language model, to see what the model reproduces or amplifies [3][4].\n</li>\n</ol>\n<h3>Context Windows</h3>\n<p>The bias score depends on what counts as “in context”:</p>\n<ol>\n<li>\n<strong>Fixed context</strong>: a symmetric window of k words on each side of <em>w</em>. They test various sizes and use k = 10 (10 words before and 10 after) as a compromise between local meaning and broader topicality [5].\n</li>\n<li>\n<strong>Infinite context with exponential decay</strong>: all occurrences in the document are considered, but closer gendered words get higher weight, decreasing exponentially with distance (base 0.95) [5]. This reduces sensitivity to arbitrary window size and emphasizes nearby context.\n</li>\n</ol>\n<h2>Language Model Setup</h2>\n<p>The authors use a <strong>three-layer AWD-LSTM</strong> (a strong recurrent neural network architecture) as a <strong>word-level language model</strong> [3][4]:</p>\n<ul>\n<li>\nEmbedding size: 400\n</li>\n<li>\nHidden units: 1150\n</li>\n<li>\nTrained in PyTorch\n</li>\n<li>\nObjective: standard cross-entropy loss to predict next word, measured by <strong>perplexity</strong> (lower perplexity = better language modeling) [3].<br>They train separate models on three corpora:\n</li>\n</ul>\n<ol>\n<li>\n<strong>Penn Treebank (PTB)</strong>: news, scientific abstracts, computer manuals; relatively male-skewed references [3].\n</li>\n<li>\n<strong>WikiText-2</strong>: Wikipedia articles, more diverse and somewhat more gender-balanced than PTB [3].\n</li>\n<li>\n<strong>CNN/Daily Mail</strong> (subsampled): news across varied topics (health, travel, entertainment, etc.); most balanced of the three in male vs. female word counts [3].<br>Baseline perplexities (no debiasing):\n</li>\n</ol>\n<ul>\n<li>\nPTB: 62.56\n</li>\n<li>\nWikiText-2: 67.67\n</li>\n<li>\nCNN/Daily Mail: 118.01 [3]<br>All are considered reasonable for these tasks.\n</li>\n</ul>\n<h2>Measuring Overall Bias and Its Amplification</h2>\n<p>To evaluate how biased a dataset or a model’s generated text is, the authors use three corpus-level statistics over all word-level bias scores [4][5][7]:</p>\n<ol>\n<li>\n<p><strong>Mean absolute bias (µ)</strong>:</p>\n<ul>\n<li>\nAverage of |bias(w)| across words.\n</li>\n<li>\nHigher µ = stronger average gender association (more biased).\n</li>\n</ul>\n</li>\n<li>\n<p><strong>Standard deviation (σ)</strong>:</p>\n<ul>\n<li>\nSpread of bias scores around zero.\n</li>\n<li>\nLower σ = more words clustered near neutral (less extreme gender associations).\n</li>\n</ul>\n</li>\n<li>\n<p><strong>Amplification factor (β)</strong>:</p>\n<ul>\n<li>\nThey fit a simple linear regression for each model:<br>[<br>\\text{bias}<em>\\lambda(w) = \\beta \\cdot \\text{bias}</em>{train}(w) + c<br>]<br>where (\\text{bias}<em>\\lambda(w)) is the bias score of word w in the </em><strong><em>generated text</em></strong><em> of a model trained with debiasing strength λ, and (\\text{bias}</em>{train}(w)) is the word’s bias in the training corpus [5].\n</li>\n<li>\nβ &gt; 1: model amplifies training bias.\n</li>\n<li>\n0 &lt; β &lt; 1: model dampens training bias.\n</li>\n<li>\nβ &lt; 0: model reverses biases (female-associated words become male-associated, and vice versa), which is also undesirable.<br>They also compare <strong>fixed</strong> vs. <strong>infinite context</strong> versions of µ, σ, and β [4].\n</li>\n</ul>\n</li>\n</ol>\n<h2>Debiasing Method: Gender Subspace Regularization</h2>\n<p>The key technical idea is to <strong>discourage word embeddings from encoding gender information</strong>, except for explicitly gendered words.</p>\n<h3>Gender Subspace</h3>\n<p>Following Bolukbasi et al. (2016), they:</p>\n<ol>\n<li>\nBuild <strong>defining pairs</strong> of words that differ only by gender, such as (“man”,”woman”), (“king”,”queen”), (“husband”,”wife”), for each corpus [5][9].\n</li>\n<li>\nFor each pair, compute the difference of their embeddings (e.g., embedding(man) – embedding(woman)).\n</li>\n<li>\nStack these difference vectors into a matrix C and perform <strong>singular value decomposition (SVD)</strong>: ( C = U \\Sigma V ).\n</li>\n<li>\nDefine the <strong>gender subspace</strong> B as the first <em>k</em> right-singular vectors (columns of V) that capture at least 50% of the variance among these difference vectors [5].<br>Intuition: This subspace encodes the main directions along which embeddings differ by gender.\n</li>\n</ol>\n<h3>Bias Regularization Term</h3>\n<p>Let:</p>\n<ul>\n<li>\nW be the word embedding matrix.\n</li>\n<li>\nN ⊂ W be the embeddings of words that should behave in a gender-neutral way.\n</li>\n<li>\nB be the gender subspace basis.<br>They penalize the squared Frobenius norm of the projection of N onto B:\n</li>\n<li>\n(L_B = \\lambda | N B |_F^2) [6]<br>Here:\n</li>\n<li>\nλ ≥ 0 is a <strong>hyperparameter</strong> controlling the strength of debiasing.\n</li>\n<li>\n<p>This penalty is added to the usual language modeling loss:</p>\n<ul>\n<li>\nTotal loss = Cross-entropy + ( L_B ).<br>They experiment with debiasing:\n</li>\n</ul>\n</li>\n<li>\n<strong>Input embeddings only</strong>\n</li>\n<li>\n<strong>Output embeddings only</strong>\n</li>\n<li>\n<strong>Both simultaneously</strong> [3]<br>They find that debiasing <strong>both</strong> input and output embeddings makes the model <strong>unstable</strong> with notably worsened perplexity, so they focus on <strong>input embeddings only</strong> in reported results [3].\n</li>\n</ul>\n<h2>Experiments: Varying Debiasing Strength (λ)</h2>\n<p>They train models at multiple λ values and generate large text samples (10⁶ tokens) per setting to compute bias metrics [6].</p>\n<h3>Corpus-Level Results</h3>\n<p>For each dataset, they report µ, σ, β (for both context definitions) and perplexity as λ increases [4]:</p>\n<h4>Penn Treebank (PTB)</h4>\n<ul>\n<li>\nTraining corpus µ (absolute mean bias): 0.83 (fixed context), 1.00 (infinite context).\n</li>\n<li>\n<p>Unregularized model (λ = 0):</p>\n<ul>\n<li>\nµ ≈ 0.74 (fixed), 0.91 (infinite), β ≈ 0.40 (fixed), 0.38 (infinite), perplexity 62.56.\n</li>\n</ul>\n</li>\n<li>\n<p>For small-to-moderate λ (e.g., 0.001–0.1):</p>\n<ul>\n<li>\nµ and σ generally <strong>decrease</strong>, indicating reduced average bias and less spread.\n</li>\n<li>\nβ stays below 0.5, suggesting <strong>no strong amplification</strong> of training bias [4][7].\n</li>\n</ul>\n</li>\n<li>\n<p>For larger λ (≥ 0.5):</p>\n<ul>\n<li>\nSome instability: µ and σ increase again and perplexity rises slightly or fluctuates (e.g., λ=0.8 → perplexity 63.36) [4].\n</li>\n</ul>\n</li>\n</ul>\n<h4>WikiText-2</h4>\n<ul>\n<li>\nTraining µ: 0.80 (fixed), 1.00 (infinite).\n</li>\n<li>\n<p>λ = 0:</p>\n<ul>\n<li>\nµ ≈ 0.70 (fixed), 0.84 (infinite), β ≈ 0.29 (fixed), 0.15 (infinite), perplexity 67.67.\n</li>\n</ul>\n</li>\n<li>\n<p>For λ around 0.01:</p>\n<ul>\n<li>\nµ and σ further <strong>decline</strong>; β also slightly decreases.\n</li>\n<li>\nPerplexity changes minimally (e.g., 67.78 vs 67.67), implying a <strong>small cost</strong> in language modeling quality [4].\n</li>\n</ul>\n</li>\n<li>\n<p>For higher λ (≥ 0.5):</p>\n<ul>\n<li>\nµ and σ rise back; β sometimes increases; perplexity degrades (e.g., ~69+), showing over-aggressive debiasing can harm performance [4].\n</li>\n</ul>\n</li>\n</ul>\n<h4>CNN/Daily Mail</h4>\n<ul>\n<li>\nTraining µ: 0.72 (fixed), 0.94 (infinite).\n</li>\n<li>\n<p>λ = 0:</p>\n<ul>\n<li>\nµ ≈ 0.51 (fixed), 0.68 (infinite), β ≈ 0.22 (fixed), 0.29 (infinite), perplexity 118.01.\n</li>\n</ul>\n</li>\n<li>\n<p>Interestingly, some moderate λ values (0.1–0.5) <strong>both</strong>:</p>\n<ul>\n<li>\nReduce µ (e.g., to 0.34–0.38) and\n</li>\n<li>\nSlightly <strong>improve</strong> perplexity (e.g., 116.19–116.49 vs. 118.01), suggesting that mild debiasing can sometimes act as a <strong>regularizer</strong> for the model [4][6].\n</li>\n</ul>\n</li>\n<li>\n<p>For larger λ (0.8–1.0):</p>\n<ul>\n<li>\nµ and σ strongly increase and perplexity worsens (up to ~121), again indicating instability and overcorrection [4].<br>Overall pattern:\n</li>\n</ul>\n</li>\n<li>\nThere is an <strong>optimal range of λ</strong> where µ and σ decrease (less overall gender association) and perplexity remains stable or even slightly improves.\n</li>\n<li>\nBeyond that range, both bias measures and perplexity can worsen.\n</li>\n</ul>\n<h3>Word-Level Examples</h3>\n<p>The paper also examines bias scores for individual words before and after debiasing to capture effects that corpus averages miss [10][11][12].<br>Examples:</p>\n<ul>\n<li>\nIn WikiText-2, many male-skewed words (e.g., “manager,” “Sir,” “university”) start with negative bias (male-associated) in training and see their magnitude reduced or partially rebalanced for moderate λ [10].\n</li>\n<li>\nFemale-associated words (e.g., “Katherine,” “partners,” “performances”) generally retain female association but the extreme scores tend to moderate at certain λ values [10][12].<br>From generated CNN/Daily Mail text snippets [6]:\n</li>\n<li>\n<p><strong>“crying”</strong> and <strong>“fragile”</strong> (stereotypically female-associated):</p>\n<ul>\n<li>\nAt λ = 0, contexts involve women and emotional situations.\n</li>\n<li>\nAt λ = 0.5 or 1.0, the gender references diversify (e.g., “he … crying”) or become more neutral; “fragile” appears in more gender-neutral or at least less obviously feminized contexts [6].\n</li>\n</ul>\n</li>\n<li>\n<p><strong>“leadership”</strong> and <strong>“prisoner(s)”</strong> (often stereotyped male):</p>\n<ul>\n<li>\nAt λ = 0, contexts tend to feature male pronouns or roles.\n</li>\n<li>\nWith debiasing (e.g., λ = 0.5), some generated passages avoid explicit gender altogether, reducing the association between these words and male pronouns [6].<br>These qualitative examples align with the quantitative finding that moderate λ values reduce the strength of gendered co-occurrence patterns, often by <strong>increasing neutral contexts</strong> rather than flipping the bias.\n</li>\n</ul>\n</li>\n</ul>\n<h2>Key Findings</h2>\n<ol>\n<li>\n<p><strong>Training corpora contain measurable gender bias.</strong></p>\n<ul>\n<li>\nPenn Treebank is the most biased (highest µ), CNN/Daily Mail the least, consistent with their content domains and gender representation [3][7].\n</li>\n</ul>\n</li>\n<li>\n<p><strong>Language models do not always strongly amplify bias.</strong></p>\n<ul>\n<li>\nFor many settings, β is below 1 (and often well below), indicating that generated text can be less biased along this metric than the training data itself [7]. This is more nuanced than previous claims that models systematically amplify bias.\n</li>\n</ul>\n</li>\n<li>\n<p><strong>Regularization via gender subspace projection can reduce measured bias.</strong></p>\n<ul>\n<li>\nAt small-to-moderate λ, both µ and σ usually decline for generated text, and word-level scores show weakened associations between stereotyped words and gendered terms [4][7][10–12].\n</li>\n</ul>\n</li>\n<li>\n<p><strong>There is a trade-off between debiasing and language modeling performance.</strong></p>\n<ul>\n<li>\nAs λ increases, perplexity generally worsens after a point, reflecting that forcing male and female words to be equally probable in contexts where the training data is skewed reduces predictive accuracy [7][8].\n</li>\n<li>\nIn CNN/Daily Mail, <strong>modest</strong> debiasing improves perplexity, suggesting a regularizing effect; but strong debiasing still harms performance [4].\n</li>\n</ul>\n</li>\n<li>\n<p><strong>Corpus-level measures (µ, σ) can be misleading by themselves.</strong></p>\n<ul>\n<li>\nWhile they indicate overall shifts, they do not show whether particular words have become less biased, more biased, or even reversed.\n</li>\n<li>\nThe regression slope β, which compares word-level bias in generated vs. training text, is a more sensitive measure of <strong>how the model alters individual word associations</strong> [7][8].\n</li>\n</ul>\n</li>\n<li>\n<p><strong>Debiasing embeddings alone does not remove all bias.</strong></p>\n<ul>\n<li>\nOther parts of the model (e.g., hidden states, output layer) can still encode and express gendered patterns.\n</li>\n<li>\nThe method mainly addresses one specific dimension of representational bias (gender direction in embeddings) and cannot guarantee overall fairness [3][8].\n</li>\n</ul>\n</li>\n</ol>\n<h2>Implications</h2>\n<ul>\n<li>\n<p><strong>For practitioners</strong>:<br>This study provides:</p>\n<ul>\n<li>\nA practical <strong>metric suite</strong> (µ, σ, β under different contexts) to assess gender bias in both corpora and language models.\n</li>\n<li>\nA <strong>training-time debiasing method</strong>—adding a soft regularization term against a learned gender subspace—that can be tuned to balance fairness and performance.\n</li>\n<li>\nEvidence that small amounts of debiasing can reduce measured gender association without major performance loss and occasionally with performance gains.\n</li>\n</ul>\n</li>\n<li>\n<p><strong>For researchers</strong>:</p>\n<ul>\n<li>\nIt empirically supports that <strong>embedding-level debiasing is partially effective but incomplete</strong>, consistent with critiques that debiasing along a single “gender direction” leaves other, more subtle structure intact [3][8].\n</li>\n<li>\nIt highlights the need for <strong>word-level and task-specific measures</strong> of bias and for multi-dimensional approaches that go beyond just removing a single gender subspace.\n</li>\n</ul>\n</li>\n<li>\n<p><strong>For broader discourse on algorithmic fairness</strong>:<br>The work underscores:</p>\n<ul>\n<li>\nHow deeply and pervasively gender bias is encoded in widely used text corpora.\n</li>\n<li>\nThat attempts to “debias” models inevitably face trade-offs with accuracy, especially in tasks that model real-world distributions which are themselves socially skewed.\n</li>\n<li>\nThat multiple, complementary bias metrics are necessary, since “removal” of bias under one metric may conceal other remaining or newly introduced biases [8].<br>In summary, the paper proposes a systematic way to <strong>detect</strong> and <strong>moderately reduce</strong> gender bias in word-level language models by penalizing reliance on a learned gender subspace in embeddings, shows that this can reduce measured bias while preserving reasonable model quality, and cautions that such methods only address a slice of the broader fairness problem in language technologies.\n</li>\n</ul>\n</li>\n</ul>\n</div>",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-28T04:10:14Z",
          "dateModified": "2025-11-28T04:11:40Z",
          "uri": "http://zotero.org/users/12031879/items/TXP5FW6J"
        }
      ],
      "citationKey": "bordiaIdentifyingReducingGender2019",
      "itemKey": "HFZQGBWY",
      "libraryID": 1,
      "select": "zotero://select/library/items/HFZQGBWY"
    },
    {
      "key": "RLS5CKNT",
      "version": 5090,
      "itemType": "journalArticle",
      "title": "Measuring and mitigating gender bias in legal contextualized language models",
      "date": "2024",
      "extra": "Citation Key: bozdag2024legalbias",
      "volume": "18",
      "pages": "1–26",
      "publicationTitle": "ACM Transactions on Knowledge Discovery from Data",
      "DOI": "10.1145/3628602",
      "issue": "4",
      "creators": [
        {
          "firstName": "Mert",
          "lastName": "Bozdag",
          "creatorType": "author"
        },
        {
          "firstName": "Neslihan",
          "lastName": "Sevim",
          "creatorType": "author"
        },
        {
          "firstName": "Alper",
          "lastName": "Koç",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T04:17:17Z",
      "dateModified": "2025-11-23T04:17:17Z",
      "uri": "http://zotero.org/users/12031879/items/RLS5CKNT",
      "itemID": 2215,
      "attachments": [],
      "notes": [],
      "citationKey": "bozdag2024legalbias",
      "itemKey": "RLS5CKNT",
      "libraryID": 1,
      "select": "zotero://select/library/items/RLS5CKNT"
    },
    {
      "key": "9SC3999A",
      "version": 5090,
      "itemType": "document",
      "title": "Locating and mitigating gender bias in large language models",
      "date": "2024",
      "extra": "Citation Key: cai2024locatingbias\narXiv: 2403.14409 [cs.CL]\nDOI: 10.48550/arXiv.2403.14409",
      "creators": [
        {
          "firstName": "Yuyang",
          "lastName": "Cai",
          "creatorType": "author"
        },
        {
          "firstName": "Daqing",
          "lastName": "Cao",
          "creatorType": "author"
        },
        {
          "firstName": "Rong",
          "lastName": "Guo",
          "creatorType": "author"
        },
        {
          "firstName": "Yuhan",
          "lastName": "Wen",
          "creatorType": "author"
        },
        {
          "firstName": "Gang",
          "lastName": "Liu",
          "creatorType": "author"
        },
        {
          "firstName": "Enhong",
          "lastName": "Chen",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T04:17:17Z",
      "dateModified": "2025-11-23T04:17:17Z",
      "uri": "http://zotero.org/users/12031879/items/9SC3999A",
      "itemID": 2216,
      "attachments": [],
      "notes": [],
      "citationKey": "cai2024locatingbias",
      "itemKey": "9SC3999A",
      "libraryID": 1,
      "select": "zotero://select/library/items/9SC3999A"
    },
    {
      "key": "74M5A5HQ",
      "version": 5151,
      "itemType": "journalArticle",
      "title": "Semantics derived automatically from language corpora contain human-like biases",
      "abstractNote": "Machines learn what people know implicitly\n            \n              AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan\n              et al.\n              now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior.\n            \n            \n              Science\n              , this issue p.\n              183\n              ; see also p.\n              133\n            \n          , \n            Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias.\n          , \n            Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",
      "date": "2017-04-14",
      "language": "en",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://www.science.org/doi/10.1126/science.aal4230",
      "accessDate": "2025-11-25T01:16:15Z",
      "volume": "356",
      "pages": "183-186",
      "publicationTitle": "Science",
      "DOI": "10.1126/science.aal4230",
      "issue": "6334",
      "journalAbbreviation": "Science",
      "ISSN": "0036-8075, 1095-9203",
      "creators": [
        {
          "firstName": "Aylin",
          "lastName": "Caliskan",
          "creatorType": "author"
        },
        {
          "firstName": "Joanna J.",
          "lastName": "Bryson",
          "creatorType": "author"
        },
        {
          "firstName": "Arvind",
          "lastName": "Narayanan",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-25T01:16:15Z",
      "dateModified": "2025-11-25T01:16:15Z",
      "uri": "http://zotero.org/users/12031879/items/74M5A5HQ",
      "itemID": 2241,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Submitted Version",
          "url": "https://arxiv.org/pdf/1608.07187",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-25T01:16:19Z",
          "dateModified": "2025-11-25T01:16:19Z",
          "uri": "http://zotero.org/users/12031879/items/I6FXIEF2",
          "path": "C:\\Users\\lilyc\\Zotero\\storage\\I6FXIEF2\\Caliskan et al. - 2017 - Semantics derived automatically from language corpora contain human-like biases.pdf",
          "select": "zotero://select/library/items/I6FXIEF2"
        }
      ],
      "notes": [
        {
          "key": "WSQMXRI9",
          "version": 5177,
          "itemType": "note",
          "parentItem": "74M5A5HQ",
          "note": "<h2 xmlns=\"http://www.w3.org/1999/xhtml\">This paper investigates whether artificial intelligence (AI) systems that learn language from large text collections (corpora) inevitably acquire human-like social biases, including prejudices about race and gender. Using standard natural language processing tools and well-known psychological measures of implicit bias, the authors show that widely used word representations encode not only factual knowledge about the world, but also the same patterns of bias found in human behavior and attitudes [1–3].<br />\nBelow is a concise overview of the study’s key ideas, methods, results, and implications.</h2>\n<h2 xmlns=\"http://www.w3.org/1999/xhtml\">Core Idea and Motivation</h2>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">Modern AI systems often rely on <strong>machine learning</strong> to derive meaning from text. A central tool in this area is the <strong>word embedding</strong>: a mathematical representation of words as points (vectors) in a high-dimensional space, where distances between vectors correspond to semantic relatedness [1, 8]. These embeddings are trained purely from text by observing which words tend to appear near each other.<br />\nThe authors ask:</p>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li>If word embeddings are trained on ordinary web text, do they inherit not just neutral knowledge (e.g., that flowers are pleasant) but also socially problematic <strong>biases</strong> (e.g., racial or gender prejudices) that humans exhibit?</li>\n<li>If so, how deep and broad are these biases, and are they separable from the “meaning” captured by language?<br />\nThey argue that because language reflects both the structure of the world and the culture that produced it, any system that learns semantics from language will also learn the regularities of human bias—including those we now regard as prejudiced [1–3, 10–12].</li>\n</ul>\n<hr xmlns=\"http://www.w3.org/1999/xhtml\" />\n<h2 xmlns=\"http://www.w3.org/1999/xhtml\">Key Concepts</h2>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>Bias vs. Prejudice</strong>:\n<ul>\n<li>In machine learning, “bias” is any prior information or regularity learned from data. It is necessary for learning.</li>\n<li>The authors reserve <strong>“prejudice”</strong> for those biases whose effects are harmful or ethically problematic; this can only be judged by their consequences, not by the algorithm alone [2].</li>\n</ul>\n</li>\n<li><strong>Word Embeddings</strong>:\n<ul>\n<li>Each word is mapped to a vector in a 300-dimensional space.</li>\n<li>Words that appear in similar contexts (e.g., “doctor,” “physician”) tend to be close together.</li>\n<li>Famous property: vector arithmetic reveals relationships (e.g., king − man + woman ≈ queen) [8].</li>\n</ul>\n</li>\n<li><strong>GloVe Model</strong>:\n<ul>\n<li>The study uses the <strong>GloVe</strong> embedding trained on the <strong>Common Crawl</strong> corpus (840 billion tokens, ~2.2 million distinct tokens) — a large sample of web text, similar to everyday language humans encounter [8].</li>\n<li>Results were also checked on another embedding (word2vec on Google News) and found to be similar [8].</li>\n</ul>\n</li>\n<li><strong>Implicit Association Test (IAT)</strong>:\n<ul>\n<li>A psychological test that measures how quickly people associate pairs of concepts (e.g., “flower + pleasant” vs. “insect + pleasant”).</li>\n<li>Faster responses are interpreted as stronger mental associations and thus as evidence of implicit bias [2].</li>\n<li>It has been widely used to study racial, gender, and other biases.</li>\n</ul>\n</li>\n</ul>\n<hr xmlns=\"http://www.w3.org/1999/xhtml\" />\n<h2 xmlns=\"http://www.w3.org/1999/xhtml\">New Methods: WEAT and WEFAT</h2>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">The authors introduce two tests that are analogues of psychological measures of bias, but applied to word embeddings rather than human subjects [1, 2, 9].</p>\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">1. Word Embedding Association Test (WEAT) [9]</h3>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">Purpose: Detect whether two sets of target words (e.g., European American vs. African American names) differ in how strongly they are associated with two sets of attribute words (e.g., “pleasant” vs. “unpleasant”).<br />\nMethod in brief:</p>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li>\n<p>Choose:</p>\n<ul>\n<li>Two <strong>target sets</strong>: X and Y (e.g., male vs. female names).</li>\n<li>Two <strong>attribute sets</strong>: A and B (e.g., family vs. career words).</li>\n</ul>\n</li>\n<li>\n<p>For each word w, compute an <strong>association score</strong>:</p>\n<ul>\n<li>How similar w is to words in A minus how similar it is to words in B (using cosine similarity between vectors).</li>\n</ul>\n</li>\n<li>\n<p>Compute a test statistic comparing the average association for X vs. Y.</p>\n</li>\n<li>\n<p>Use a <strong>permutation test</strong> (reassigning words randomly to X/Y many times) to estimate:</p>\n<ul>\n<li>A <strong>p-value</strong> (whether the observed difference could arise by chance).</li>\n<li>An <strong>effect size</strong> (how strongly separated the two groups’ association scores are).<br />\nInterpretation:</li>\n</ul>\n</li>\n<li>\n<p>WEAT does not measure individual people’s attitudes but instead the “aggregate” association patterns encoded across the language corpus. The “subjects” are words, not people [3].</p>\n</li>\n</ul>\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">2. Word Embedding Factual Association Test (WEFAT) [9–10]</h3>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">Purpose: Test whether word embeddings encode <strong>real-world facts</strong> (e.g., what fraction of nurses are women, or how often a name is female) and not only attitudes.<br />\nMethod in brief:</p>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li>For each word w in a set W (e.g., occupations or names), we have a numeric property p_w (e.g., % of workers who are women).</li>\n<li>Compute a normalized association score of w with two attribute sets (e.g., “male” vs. “female” words).</li>\n<li>Use <strong>linear regression</strong> to see how well these association scores predict the real-world property p_w.<br />\nThis allows the authors to ask:<br />\nDo patterns in word embeddings track actual demographic statistics (like gender distribution in occupations) derived from external sources?</li>\n</ul>\n<hr xmlns=\"http://www.w3.org/1999/xhtml\" />\n<h2 xmlns=\"http://www.w3.org/1999/xhtml\">Main Empirical Results</h2>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">The authors systematically check whether GloVe reproduces a range of known human biases from psychology and sociology. In every case they investigated, they found strong and statistically significant parallels between human biases (as measured in lab or field studies) and associations in word embeddings [3].</p>\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">1. Baseline, “Neutral” Associations (Validation)</h3>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">To validate their methods, they start with associations that are morally uncontroversial and strongly documented in the IAT literature.</p>\n<h4 xmlns=\"http://www.w3.org/1999/xhtml\">Flowers vs. Insects (pleasantness) [3–4]</h4>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li>Human IAT finding: People associate <strong>flowers</strong> with “pleasant” and <strong>insects</strong> with “unpleasant” (large effect size, p ≈ 10⁻⁸).</li>\n<li>Embedding result (WEAT on GloVe):\n<ul>\n<li>Flower words (e.g., “rose,” “tulip”) are closer to pleasant words than insect words (e.g., “cockroach,” “spider”).</li>\n<li>Effect size: 1.50 (very large), p &lt; 10⁻⁷ [3–4].</li>\n</ul>\n</li>\n<li>Interpretation: The embedding captures the intuitive positive/negative valence of these categories purely from text.</li>\n</ul>\n<h4 xmlns=\"http://www.w3.org/1999/xhtml\">Musical Instruments vs. Weapons (pleasantness) [4]</h4>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li>Human IAT: <strong>Musical instruments</strong> are associated more with pleasant words than <strong>weapons</strong>; large effect size, p ≈ 10⁻¹⁰.</li>\n<li>Embedding result:\n<ul>\n<li>Instrument words (e.g., “guitar,” “violin”) are more associated with pleasantness than weapon words (e.g., “gun,” “knife”).</li>\n<li>Effect size: 1.53, p &lt; 10⁻⁷ [4].</li>\n</ul>\n</li>\n<li>Again, the embedding reflects the expected valence pattern.<br />\nThese baseline replications demonstrate that word embeddings reliably encode widely shared, relatively value-neutral associations.</li>\n</ul>\n<hr xmlns=\"http://www.w3.org/1999/xhtml\" />\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">2. Racial Biases</h3>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">The study then turns to racial bias, starting with standard IAT results and a well-known field experiment on hiring discrimination.</p>\n<h4 xmlns=\"http://www.w3.org/1999/xhtml\">Race and Valence (Names) [4–5]</h4>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li>Human IAT (Greenwald et al., 1998):\n<ul>\n<li><strong>European American</strong> names (e.g., “Adam,” “Amanda”) are more easily associated with “pleasant” words than <strong>African American</strong> names (e.g., “Jamal,” “Lakisha”), with large effect size and very small p-value.</li>\n</ul>\n</li>\n<li>Embedding result (using slightly trimmed name lists due to rarity in the corpus):\n<ul>\n<li>European American name vectors are more associated with pleasant than unpleasant words, compared to African American name vectors.</li>\n<li>Effect size: 1.41, p &lt; 10⁻⁸ [4–5].<br />\nThis parallels the racial bias seen in the psychological tests, but exists here as a pattern in text-derived vectors.</li>\n</ul>\n</li>\n</ul>\n<h4 xmlns=\"http://www.w3.org/1999/xhtml\">Replicating the Bertrand &amp; Mullainathan Résumé Study [5]</h4>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li>Original study (field experiment, 2004):\n<ul>\n<li>Researchers sent ~5,000 identical résumés to employers, varying only the name to signal race (e.g., “Emily/Greg” vs. “Lakisha/Jamal”).</li>\n<li>Résumés with European American names received <strong>50% more callbacks</strong> for interviews.</li>\n</ul>\n</li>\n<li>Embedding assumption: Words or names closer to “pleasant” than “unpleasant” are more likely to receive positive treatment (e.g., callbacks).</li>\n<li>Embedding results:\n<ul>\n<li>Using the same sets of European American and African American names, the authors again find that European American names are closer to pleasantness.</li>\n<li>With original IAT pleasant/unpleasant words: effect size 1.50, p &lt; 10⁻⁴.</li>\n<li>With an updated, shorter pleasant/unpleasant list: effect size 1.28, p &lt; 10⁻³ [5].</li>\n</ul>\n</li>\n<li>Implication: The same <strong>linguistic associations</strong> that embeddings learn from text align with real-world discriminatory patterns in hiring, suggesting that historical biases in practice are reflected in language and thus learned by AI.</li>\n</ul>\n<hr xmlns=\"http://www.w3.org/1999/xhtml\" />\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">3. Gender Biases</h3>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">The paper examines how word embeddings encode gender stereotypes, again by replicating IAT-style biases and comparing them to real-world data.</p>\n<h4 xmlns=\"http://www.w3.org/1999/xhtml\">Gender vs. Career vs. Family [5]</h4>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li>Human IAT (Nosek et al., 2002a; ~38,800 online participants):\n<ul>\n<li><strong>Female</strong> names are more associated with <strong>family</strong> words (e.g., “home,” “children”), and <strong>male</strong> names with <strong>career</strong> words (e.g., “executive,” “salary”).</li>\n<li>Effect size: 0.72.</li>\n</ul>\n</li>\n<li>Embedding result:\n<ul>\n<li>In GloVe, female names are more strongly associated with family words, male names with career words.</li>\n<li>Effect size: 1.81, p &lt; 10⁻³ [5–6].</li>\n</ul>\n</li>\n<li>Interpretation: Despite no direct encoding of gender roles, the embedding reflects entrenched cultural associations of women with domestic life and men with work.</li>\n</ul>\n<h4 xmlns=\"http://www.w3.org/1999/xhtml\">Gender vs. Mathematics vs. Arts [6]</h4>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li>Human IAT (Nosek et al., 2002a; ~28,000 participants):\n<ul>\n<li>Female terms are more associated with <strong>arts</strong> than with <strong>mathematics</strong>, while male terms are relatively more associated with math.</li>\n<li>Effect size: 0.82.</li>\n</ul>\n</li>\n<li>Embedding result:\n<ul>\n<li>Words for “female” are more associated with arts terms (e.g., “poetry,” “dance”) than with math terms (e.g., “algebra,” “geometry”), relative to male terms.</li>\n<li>Effect size: 1.06, p ≈ 10⁻² [6].</li>\n</ul>\n</li>\n</ul>\n<h4 xmlns=\"http://www.w3.org/1999/xhtml\">Gender vs. Arts vs. Sciences [6]</h4>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li>Human IAT (Nosek et al., 2002b):\n<ul>\n<li>Female terms less associated with <strong>science</strong>, male terms less associated with <strong>art/language</strong>, with very large effect size and tiny p-value.</li>\n</ul>\n</li>\n<li>Embedding result:\n<ul>\n<li>Female terms more associated with arts, male terms with science (e.g., “physics,” “chemistry,” “Einstein”).</li>\n<li>Effect size: 1.24, p ≈ 10⁻² [6].<br />\nThese results show that word embeddings encode widely discussed gender–STEM stereotypes and gender–family role stereotypes.</li>\n</ul>\n</li>\n</ul>\n<hr xmlns=\"http://www.w3.org/1999/xhtml\" />\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">4. Comparison to Real-World Data</h3>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">To demonstrate that word embeddings encode <strong>factual patterns</strong> as well as stereotypical “attitudes,” the authors apply WEFAT to real statistics.</p>\n<h4 xmlns=\"http://www.w3.org/1999/xhtml\">Occupations and Gender Composition [6–7, 9–10]</h4>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li>Real-world data: 2015 U.S. Bureau of Labor Statistics (BLS) occupational data, including the percentage of workers in each occupation who are women [6].</li>\n<li>Approach:\n<ul>\n<li>Convert multi-word occupational labels (e.g., “chemical engineer”) into single-word categories (“engineer”) where possible.</li>\n<li>Use gender words (e.g., “he,” “she,” “man,” “woman”) as attributes and apply WEFAT.</li>\n</ul>\n</li>\n<li>Result:\n<ul>\n<li>The association between an occupation word and “female” vs. “male” predicts the actual percentage of women in that occupation extremely well.</li>\n<li>Pearson correlation coefficient ρ = 0.90, p &lt; 10⁻¹⁸ [6–7, 9–10].</li>\n</ul>\n</li>\n<li>Interpretation:\n<ul>\n<li>Word embeddings don’t just reflect stereotypes; they encode <strong>veridical demographic information</strong> about gender segregation in occupations as it appears in language.</li>\n</ul>\n</li>\n</ul>\n<h4 xmlns=\"http://www.w3.org/1999/xhtml\">Androgynous Names and Gender Proportions [7–8, 9–10]</h4>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li>Real-world data: 1990 U.S. Census figures on first names and their gender distributions.</li>\n<li>Approach:\n<ul>\n<li>Select commonly used <strong>androgynous names</strong> (e.g., “Kelly,” “Taylor,” “Jordan”) that can be male or female.</li>\n<li>Remove names that are overly ambiguous because they are also common English words (e.g., “Will”), by filtering out names whose vectors are least “name-like” [9–10].</li>\n<li>Use gender word sets as attributes and apply WEFAT.</li>\n</ul>\n</li>\n<li>Result:\n<ul>\n<li>The gender association of each name’s vector predicts what fraction of people with that name are women.</li>\n<li>Pearson ρ = 0.84, p &lt; 10⁻¹³ [7–8, 9–10].</li>\n</ul>\n</li>\n<li>This further supports the view that word embeddings mirror real-world statistical regularities learned from language.</li>\n</ul>\n<hr xmlns=\"http://www.w3.org/1999/xhtml\" />\n<h2 xmlns=\"http://www.w3.org/1999/xhtml\">Interpretation and Broader Implications</h2>\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">1. Bias is Bound Up with Meaning</h3>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">A central claim of the paper is that in language-based AI, <strong>bias and meaning are inseparable</strong>:</p>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li>The same statistical patterns that capture benign or useful information (e.g., that nurses are typically women, or that roses are pleasant) also encode harmful prejudices (e.g., that certain ethnic names are less “pleasant” or less employable) [2, 10–12].</li>\n<li>Because embeddings learn from <strong>co-occurrence patterns</strong> in text, they internalize both:\n<ul>\n<li>Factual structures of the world (e.g., occupational gender distributions), and</li>\n<li>Cultural norms and historical injustices (e.g., systematic discrimination reflected in how groups are talked about).<br />\nTherefore, trying to remove all “bias” from embeddings would also strip them of much of their semantic content [12].</li>\n</ul>\n</li>\n</ul>\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">2. A New “Null Hypothesis” for Human Prejudice</h3>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">The authors argue that their findings have implications beyond AI, for understanding human prejudice itself:</p>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li>Language alone may be sufficient to transmit many social biases, even without explicit teaching or deliberate malice.</li>\n<li>This suggests a powerful, <strong>parsimonious “null hypothesis”</strong>: before developing complex explanations for prejudice (e.g., elaborate theories of institutional racism or intergroup conflict), we should first ask how much can be accounted for simply by <strong>absorbing the structure of language</strong> present in one’s culture [2, 10–11].</li>\n<li>Their results also align with theories that <strong>ingroup favoritism</strong>, rather than explicit hostility, may suffice to create discriminatory outcomes [10–11].<br />\nThis does not excuse prejudice; instead, it reframes some of its origins as “unthinking reproduction of statistical regularities” learned from language, which then must be consciously corrected.</li>\n</ul>\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">3. Consequences for AI Safety and Fairness</h3>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">The study shows that:</p>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li>AI systems that use word embeddings (or related language models) <strong>cannot be assumed neutral</strong>. They inherit the full spectrum of biases from their training data [1–3, 10–11].</li>\n<li>Because AI is increasingly used in decision contexts (e.g., hiring, criminal sentencing, recommendations, translation), such latent biases can influence outcomes in ways that are:\n<ul>\n<li>Hard to detect,</li>\n<li>Systematic, and</li>\n<li>Potentially long-lasting (once “locked in” to deployed models) [10–11].<br />\nThe authors highlight examples:</li>\n</ul>\n</li>\n<li><strong>Sentiment Analysis</strong>:\n<ul>\n<li>If a naive sentiment system bases positivity on associations with “pleasant” words, then sentences mentioning European American names may be rated more positively than those with African American names, all else equal [11].</li>\n</ul>\n</li>\n<li><strong>Machine Translation</strong>:\n<ul>\n<li>Translating from gender-neutral languages (like Turkish) into English can inject gender stereotypes. For example, Google Translate tends to output “He is a doctor” and “She is a nurse” from a gender-neutral Turkish sentence, and the choice of “he” or “she” correlates strongly with the gender association of the occupation word in the embedding [11].<br />\nThese examples show how biases encoded in embeddings can surface in widely used products.</li>\n</ul>\n</li>\n</ul>\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">4. Why “Debiasing” Embeddings Is Not a Complete Solution</h3>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">Some work proposes removing explicit gender or racial dimensions from embeddings (“debiasing”). The authors are skeptical:</p>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>Fairness through blindness</strong>: Simply erasing certain correlations from the representation amounts to giving the AI a <strong>distorted view of reality</strong>, which might:\n<ul>\n<li>Reduce the system’s overall accuracy and semantic richness, and</li>\n<li>Still allow prejudice to re-enter via proxy variables (other correlated features) [12].</li>\n</ul>\n</li>\n<li>Societal understanding of what counts as <strong>prejudiced</strong> is evolving and context-dependent. It is hard to encode a timeless, fully general rule specifying which associations should be removed [12].</li>\n<li>Many associations are simultaneously:\n<ul>\n<li>Prejudicial in one context (e.g., using gendered occupation stereotypes to screen job applicants), but</li>\n<li>Useful and accurate in another (e.g., inferring historical gender distributions in labor statistics) [12].<br />\nThey argue that <strong>remedies must be tailored to the specific application and context</strong>, rather than trying to “cleanse” the underlying representation once and for all.</li>\n</ul>\n</li>\n</ul>\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">5. Toward More Responsible AI</h3>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">The authors advocate for:</p>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>Awareness rather than blindness</strong>:\n<ul>\n<li>Make biases in language models explicit, measurable, and monitored (using tools like WEAT and WEFAT).</li>\n<li>Design algorithms that recognize these biases and <strong>compensate at the decision-making stage</strong>, much as humans can learn moral rules to counteract implicit biases [12].</li>\n</ul>\n</li>\n<li><strong>Richer AI architectures</strong>:\n<ul>\n<li>Consider systems that integrate statistical learning (like embeddings) with more explicit, symbolic reasoning and ethical constraints—sometimes called <strong>cognitive systems</strong>—to allow deliberate override of prejudiced inferences [12].</li>\n</ul>\n</li>\n<li><strong>Careful corpus selection</strong>:\n<ul>\n<li>Choose training data that minimizes prejudicial content where possible, and assess corpora using these tests before deploying embeddings in sensitive applications [12].</li>\n</ul>\n</li>\n<li><strong>Interdisciplinary collaboration</strong>:\n<ul>\n<li>Addressing these issues requires contributions from computer scientists, cognitive scientists, psychologists, sociologists, ethicists, and legal scholars [10, 12].</li>\n</ul>\n</li>\n</ul>\n<hr xmlns=\"http://www.w3.org/1999/xhtml\" />\n<h2 xmlns=\"http://www.w3.org/1999/xhtml\">Overall Significance</h2>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">This study demonstrates that:</p>\n<ol xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>Standard machine learning methods trained on ordinary web text automatically acquire a wide range of human-like biases</strong>, including ethically problematic ones related to race and gender, without any explicit programming of these biases [1–3].</li>\n<li>The same statistical structure that enables word embeddings to capture <strong>meaning</strong> also encodes <strong>bias</strong>; in this setting, “bias is meaning” [12].</li>\n<li>AI systems built on such representations are thus <strong>not neutral</strong> and can perpetuate or even exacerbate societal inequities if deployed without awareness and mitigation.</li>\n<li>The findings also reshape how we understand human prejudice by showing that simple exposure to language may be enough to transmit many biases, suggesting a powerful baseline explanation for the persistence of stereotypes [10–11].<br />\nIn sum, the paper provides both empirical evidence and conceptual tools to understand how cultural biases become embedded in AI, and argues that addressing prejudice in intelligent systems requires more than just technical fixes: it requires explicit ethical design, transparency, context-sensitive interventions, and interdisciplinary research.</li>\n</ol>",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-28T04:01:52Z",
          "dateModified": "2025-11-28T04:01:52Z",
          "uri": "http://zotero.org/users/12031879/items/WSQMXRI9"
        }
      ],
      "citationKey": "caliskanSemanticsDerivedAutomatically2017",
      "itemKey": "74M5A5HQ",
      "libraryID": 1,
      "select": "zotero://select/library/items/74M5A5HQ"
    },
    {
      "key": "GIW4BJAH",
      "version": 5025,
      "itemType": "conferencePaper",
      "title": "How do teachers' gender stereotypes impact students?",
      "date": "2022",
      "url": "https://doi.org/10.2991/assehr.k.220131.133",
      "extra": "Citation Key: dai_how_2022",
      "publisher": "Atlantis Press",
      "pages": "728–733",
      "proceedingsTitle": "Proceedings of the 2021 international conference on education, language and art (ICELA 2021)",
      "DOI": "10.2991/assehr.k.220131.133",
      "creators": [
        {
          "firstName": "Anqi",
          "lastName": "Dai",
          "creatorType": "author"
        },
        {
          "firstName": "Danni",
          "lastName": "Li",
          "creatorType": "author"
        },
        {
          "firstName": "Hongxu",
          "lastName": "Zhu",
          "creatorType": "author"
        },
        {
          "firstName": "Zihan",
          "lastName": "Zhang",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T03:44:20Z",
      "dateModified": "2025-11-23T03:44:20Z",
      "uri": "http://zotero.org/users/12031879/items/GIW4BJAH",
      "itemID": 2173,
      "attachments": [],
      "notes": [],
      "citationKey": "dai_how_2022",
      "itemKey": "GIW4BJAH",
      "libraryID": 1,
      "select": "zotero://select/library/items/GIW4BJAH"
    },
    {
      "key": "UVUGVUK5",
      "version": 5024,
      "itemType": "journalArticle",
      "title": "Persistence of gender biases in Europe",
      "date": "2023",
      "url": "https://www.pnas.org/doi/10.1073/pnas.2213266120",
      "extra": "Citation Key: damann_persistence_2023",
      "volume": "120",
      "pages": "e2213266120",
      "publicationTitle": "Proceedings of the National Academy of Sciences",
      "DOI": "10.1073/pnas.2213266120",
      "issue": "12",
      "creators": [
        {
          "firstName": "Taylor J.",
          "lastName": "Damann",
          "creatorType": "author"
        },
        {
          "firstName": "Jeremy",
          "lastName": "Siow",
          "creatorType": "author"
        },
        {
          "firstName": "Margit",
          "lastName": "Tavits",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T03:44:20Z",
      "dateModified": "2025-11-23T03:44:20Z",
      "uri": "http://zotero.org/users/12031879/items/UVUGVUK5",
      "itemID": 2163,
      "attachments": [],
      "notes": [],
      "citationKey": "damann_persistence_2023",
      "itemKey": "UVUGVUK5",
      "libraryID": 1,
      "select": "zotero://select/library/items/UVUGVUK5"
    },
    {
      "key": "3ERFCJ6P",
      "version": 5161,
      "itemType": "preprint",
      "title": "Leveraging Large Language Models to Measure Gender Representation Bias in Gendered Language Corpora",
      "abstractNote": "Large language models (LLMs) often inherit and amplify social biases embedded in their training data. A prominent social bias is gender bias. In this regard, prior work has mainly focused on gender stereotyping bias - the association of specific roles or traits with a particular gender - in English and on evaluating gender bias in model embeddings or generated outputs. In contrast, gender representation bias - the unequal frequency of references to individuals of different genders - in the training corpora has received less attention. Yet such imbalances in the training data constitute an upstream source of bias that can propagate and intensify throughout the entire model lifecycle. To fill this gap, we propose a novel LLM-based method to detect and quantify gender representation bias in LLM training data in gendered languages, where grammatical gender challenges the applicability of methods developed for English. By leveraging the LLMs' contextual understanding, our approach automatically identifies and classifies person-referencing words in gendered language corpora. Applied to four Spanish-English benchmarks and five Valencian corpora, our method reveals substantial male-dominant imbalances. We show that such biases in training data affect model outputs, but can surprisingly be mitigated leveraging small-scale training on datasets that are biased towards the opposite gender. Our findings highlight the need for corpus-level gender bias analysis in multilingual NLP. We make our code and data publicly available.",
      "date": "2024",
      "libraryCatalog": "DOI.org (Datacite)",
      "url": "https://arxiv.org/abs/2406.13677",
      "accessDate": "2025-11-26T23:23:43Z",
      "rights": "Creative Commons Attribution 4.0 International",
      "extra": "Version Number: 3",
      "DOI": "10.48550/ARXIV.2406.13677",
      "repository": "arXiv",
      "creators": [
        {
          "firstName": "Erik",
          "lastName": "Derner",
          "creatorType": "author"
        },
        {
          "firstName": "Sara Sansalvador",
          "lastName": "de la Fuente",
          "creatorType": "author"
        },
        {
          "firstName": "Yoan",
          "lastName": "Gutiérrez",
          "creatorType": "author"
        },
        {
          "firstName": "Paloma",
          "lastName": "Moreda",
          "creatorType": "author"
        },
        {
          "firstName": "Nuria",
          "lastName": "Oliver",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computation and Language (cs.CL)",
          "type": 1
        },
        {
          "tag": "Computers and Society (cs.CY)",
          "type": 1
        },
        {
          "tag": "FOS: Computer and information sciences",
          "type": 1
        }
      ],
      "relations": {},
      "dateAdded": "2025-11-26T23:23:43Z",
      "dateModified": "2025-11-26T23:23:43Z",
      "uri": "http://zotero.org/users/12031879/items/3ERFCJ6P",
      "itemID": 2265,
      "attachments": [],
      "notes": [
        {
          "key": "CICVGNXC",
          "version": 5161,
          "itemType": "note",
          "parentItem": "3ERFCJ6P",
          "note": "<h2>Other</h2>\nAccepted for presentation at the 6th Workshop on Gender Bias in Natural Language Processing (GeBNLP) at ACL 2025",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-26T23:23:43Z",
          "dateModified": "2025-11-26T23:23:43Z",
          "uri": "http://zotero.org/users/12031879/items/CICVGNXC"
        }
      ],
      "citationKey": "dernerLeveragingLargeLanguage2024",
      "itemKey": "3ERFCJ6P",
      "libraryID": 1,
      "select": "zotero://select/library/items/3ERFCJ6P"
    },
    {
      "key": "EZZ7FF2C",
      "version": 5160,
      "itemType": "conferencePaper",
      "title": "Gender Bias in Large Language Models across Multiple Languages: A Case Study of ChatGPT",
      "date": "2025",
      "language": "en",
      "shortTitle": "Gender Bias in Large Language Models across Multiple Languages",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://aclanthology.org/2025.trustnlp-main.36",
      "accessDate": "2025-11-26T23:23:14Z",
      "place": "Albuquerque, New Mexico",
      "publisher": "Association for Computational Linguistics",
      "pages": "552-579",
      "proceedingsTitle": "Proceedings of the 5th Workshop on Trustworthy NLP (TrustNLP 2025)",
      "conferenceName": "Proceedings of the 5th Workshop on Trustworthy NLP (TrustNLP 2025)",
      "DOI": "10.18653/v1/2025.trustnlp-main.36",
      "creators": [
        {
          "firstName": "YiTian",
          "lastName": "Ding",
          "creatorType": "author"
        },
        {
          "firstName": "Jinman",
          "lastName": "Zhao",
          "creatorType": "author"
        },
        {
          "firstName": "Chen",
          "lastName": "Jia",
          "creatorType": "author"
        },
        {
          "firstName": "Yining",
          "lastName": "Wang",
          "creatorType": "author"
        },
        {
          "firstName": "Zifan",
          "lastName": "Qian",
          "creatorType": "author"
        },
        {
          "firstName": "Weizhe",
          "lastName": "Chen",
          "creatorType": "author"
        },
        {
          "firstName": "Xingyu",
          "lastName": "Yue",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-26T23:23:14Z",
      "dateModified": "2025-11-26T23:23:14Z",
      "uri": "http://zotero.org/users/12031879/items/EZZ7FF2C",
      "itemID": 2264,
      "attachments": [],
      "notes": [],
      "citationKey": "dingGenderBiasLarge2025",
      "itemKey": "EZZ7FF2C",
      "libraryID": 1,
      "select": "zotero://select/library/items/EZZ7FF2C"
    },
    {
      "key": "3GZ7WEZ3",
      "version": 5025,
      "itemType": "conferencePaper",
      "title": "Gender bias in text: Origin, taxonomy, and implications",
      "date": "2021",
      "url": "https://aclanthology.org/2021.gebnlp-1.5",
      "extra": "Citation Key: doughman_gender_2021",
      "publisher": "Association for Computational Linguistics",
      "pages": "34–44",
      "proceedingsTitle": "Proceedings of the 3rd workshop on gender bias in natural language processing",
      "DOI": "10.18653/v1/2021.gebnlp-1.5",
      "creators": [
        {
          "firstName": "Jad",
          "lastName": "Doughman",
          "creatorType": "author"
        },
        {
          "firstName": "Wael",
          "lastName": "Khreich",
          "creatorType": "author"
        },
        {
          "firstName": "Maya",
          "lastName": "El Gharib",
          "creatorType": "author"
        },
        {
          "firstName": "Maha",
          "lastName": "Wiss",
          "creatorType": "author"
        },
        {
          "firstName": "Zahraa",
          "lastName": "Berjawi",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T03:44:20Z",
      "dateModified": "2025-11-23T03:44:20Z",
      "uri": "http://zotero.org/users/12031879/items/3GZ7WEZ3",
      "itemID": 2172,
      "attachments": [],
      "notes": [],
      "citationKey": "doughman_gender_2021",
      "itemKey": "3GZ7WEZ3",
      "libraryID": 1,
      "select": "zotero://select/library/items/3GZ7WEZ3"
    },
    {
      "key": "TJEMXLJ4",
      "version": 5110,
      "itemType": "journalArticle",
      "title": "Bias and Fairness in Large Language Models: A Survey",
      "abstractNote": "Abstract\n            Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.",
      "date": "2024-09-01",
      "language": "en",
      "shortTitle": "Bias and Fairness in Large Language Models",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://direct.mit.edu/coli/article/50/3/1097/121961/Bias-and-Fairness-in-Large-Language-Models-A",
      "accessDate": "2025-11-23T04:21:39Z",
      "volume": "50",
      "pages": "1097-1179",
      "publicationTitle": "Computational Linguistics",
      "DOI": "10.1162/coli_a_00524",
      "issue": "3",
      "ISSN": "0891-2017, 1530-9312",
      "creators": [
        {
          "firstName": "Isabel O.",
          "lastName": "Gallegos",
          "creatorType": "author"
        },
        {
          "firstName": "Ryan A.",
          "lastName": "Rossi",
          "creatorType": "author"
        },
        {
          "firstName": "Joe",
          "lastName": "Barrow",
          "creatorType": "author"
        },
        {
          "firstName": "Md Mehrab",
          "lastName": "Tanjim",
          "creatorType": "author"
        },
        {
          "firstName": "Sungchul",
          "lastName": "Kim",
          "creatorType": "author"
        },
        {
          "firstName": "Franck",
          "lastName": "Dernoncourt",
          "creatorType": "author"
        },
        {
          "firstName": "Tong",
          "lastName": "Yu",
          "creatorType": "author"
        },
        {
          "firstName": "Ruiyi",
          "lastName": "Zhang",
          "creatorType": "author"
        },
        {
          "firstName": "Nesreen K.",
          "lastName": "Ahmed",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T04:21:39Z",
      "dateModified": "2025-11-23T04:21:39Z",
      "uri": "http://zotero.org/users/12031879/items/TJEMXLJ4",
      "itemID": 2234,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Full Text",
          "url": "https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli_a_00524/2381177/coli_a_00524.pdf",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-23T04:21:41Z",
          "dateModified": "2025-11-23T04:21:41Z",
          "uri": "http://zotero.org/users/12031879/items/3ITW9J5X",
          "path": "C:\\Users\\lilyc\\Zotero\\storage\\3ITW9J5X\\Gallegos et al. - 2024 - Bias and Fairness in Large Language Models A Survey.pdf",
          "select": "zotero://select/library/items/3ITW9J5X"
        }
      ],
      "notes": [
        {
          "key": "NYU4N6S2",
          "version": 5179,
          "itemType": "note",
          "parentItem": "TJEMXLJ4",
          "note": "<h2 xmlns=\"http://www.w3.org/1999/xhtml\">This paper is a comprehensive survey of how large language models (LLMs) can exhibit social bias, how such bias is measured, and what technical methods exist to mitigate it [1–5]. It brings together work from machine learning, natural language processing (NLP), and sociolinguistics, and proposes unified frameworks and taxonomies for thinking about bias and fairness in LLMs.<br />\nBelow is a concise, self-contained overview.</h2>\n<h2 xmlns=\"http://www.w3.org/1999/xhtml\">Background and Motivation</h2>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">Modern LLMs (e.g., GPT, BERT, T5) are trained on massive amounts of largely uncurated Internet text and then adapted to many tasks via prompting or fine-tuning [2,5]. This has enabled impressive performance in tasks like text generation, classification, and question answering, often in few‑ or zero‑shot settings.<br />\nHowever, because LLMs learn from human-produced data, they also absorb and can amplify harmful social biases—stereotypes, slurs, misrepresentations, and exclusionary norms—especially against marginalized groups [2,3]. These harms can appear in:</p>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li>Toxic or derogatory outputs towards certain groups,</li>\n<li>Stereotypical associations (e.g., linking “Muslim” to “terrorist”),</li>\n<li>Poor performance on some dialects (e.g., African-American English),</li>\n<li>Systematic under-representation or erasure of certain identities [2,3].<br />\nThe paper surveys how researchers (1) define and conceptualize these harms, (2) evaluate them using metrics and datasets, and (3) try to mitigate them using technical interventions throughout the LLM pipeline [2,4].</li>\n</ul>\n<hr xmlns=\"http://www.w3.org/1999/xhtml\" />\n<h2 xmlns=\"http://www.w3.org/1999/xhtml\">Key Concepts: Social Bias and Fairness</h2>\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">Social Groups and Protected Attributes</h3>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li>A <strong>social group</strong> is any subset of the population sharing an identity trait (e.g., race, gender, religion, disability) [7].</li>\n<li>A <strong>protected attribute</strong> is the trait that defines group membership (e.g., “race” or “gender”) [4].<br />\nThe authors stress that many social groups are <strong>socially constructed</strong>—their boundaries are fluid, contested, and rooted in power relations—not just neutral categories [4].</li>\n</ul>\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">Social Bias</h3>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">The paper defines <strong>social bias</strong> as disparities in treatment or outcomes between social groups that stem from historical and structural power asymmetries [7]. In NLP, this is organized into two main categories with several subtypes [7, Table 1]:</p>\n<ol xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>Representational Harms</strong> – how groups are depicted in language:\n<ul>\n<li><strong>Derogatory language</strong>: slurs or insults (e.g., “whore” used contemptuously about women) [7].</li>\n<li><strong>Disparate system performance</strong>: worse recognition or quality for some groups (e.g., African-American English misclassified as “not English” more often than Standard American English) [7].</li>\n<li><strong>Erasure</strong>: omission or minimizing of some groups’ experiences (e.g., “All lives matter” obscuring “Black lives matter”) [7].</li>\n<li><strong>Exclusionary norms</strong>: treating one group as the default (e.g., “both genders” erasing non-binary people) [7].</li>\n<li><strong>Misrepresentation</strong>: incomplete or distorted portrayals (e.g., treating autism as inherently negative) [7].</li>\n<li><strong>Stereotyping</strong>: generalized, often negative abstractions (e.g., equating “Muslim” with “terrorist”) [7].</li>\n<li><strong>Toxicity</strong>: hateful or violent language aimed at groups [7].</li>\n</ul>\n</li>\n<li><strong>Allocational Harms</strong> – how resources and opportunities are distributed:\n<ul>\n<li><strong>Direct discrimination</strong>: explicit differential treatment (e.g., LLM-aided resume screening replicating biased hiring) [7].</li>\n<li><strong>Indirect discrimination</strong>: disparate impact via “neutral” factors that act as proxies for protected attributes (e.g., biased healthcare tools) [7].<br />\nThese harms are overlapping, not mutually exclusive, and often mutually reinforcing [7].</li>\n</ul>\n</li>\n</ol>\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">Fairness Frameworks</h3>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">The paper adapts classic fairness notions to LLM settings [5,6]:</p>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>Group Fairness</strong>: outcomes should be similar across social groups (e.g., accuracy or error rates differ by at most some small ε between groups) [5].</li>\n<li><strong>Subgroup Fairness</strong>: extends group fairness to overlapping and intersectional subgroups (e.g., Black women vs. women overall) [5].</li>\n<li><strong>Individual Fairness</strong>: “similar individuals should be treated similarly,” formalized as similar inputs receiving similar output distributions under some distance metric [6].<br />\nThese are <strong>frameworks</strong>, not complete definitions: they require specifying <em>what</em> outcome measure you care about (accuracy, toxicity, sentiment, etc.), and they may conflict with one another.</li>\n</ul>\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">Fairness Desiderata for LLMs</h3>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">Because LLMs often don’t output simple class labels, the authors define several fairness “desiderata” specialized to language tasks [2.3]:</p>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>Fairness through Unawareness</strong>: the model behaves the same whether or not explicit social-group markers are present in the input [8].</li>\n<li><strong>Invariance</strong>: outputs do not change (under some metric ψ) when you swap one social group term for another (e.g., “man” → “woman”) in otherwise identical inputs [9].</li>\n<li><strong>Equal Social Group Associations</strong>: neutral words (e.g., professions) are equally likely across different groups (e.g., “engineer” is equally associated with men and women) [10].</li>\n<li><strong>Equal Neutral Associations</strong>: group-specific words (e.g., “he,” “she”) are equally likely in neutral contexts [11].</li>\n<li><strong>Replicated Distributions</strong>: the model’s conditional distribution of neutral words given groups matches a reference distribution (e.g., from a curated dataset) [12].<br />\nThe paper notes these desiderata are inherently normative choices: depending on the context, it may or may not be desirable to enforce complete invariance or equal association (e.g., some real group–language associations encode important, non-stereotypical information) [2.3, 3.6].</li>\n</ul>\n<hr xmlns=\"http://www.w3.org/1999/xhtml\" />\n<h2 xmlns=\"http://www.w3.org/1999/xhtml\">Where Bias Enters the LLM Lifecycle</h2>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">Bias can arise or be amplified at multiple points [2.2.3]:</p>\n<ol xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>Training Data</strong>:\n<ul>\n<li>Non-representative samples (over/under-representation of some groups),</li>\n<li>Inaccurate labels or proxies (e.g., “sentiment” mistaken for “harm”),</li>\n<li>Aggregation that hides important subgroup differences.<br />\nEven carefully collected data still reflects existing societal inequities.</li>\n</ul>\n</li>\n<li><strong>Model and Training Procedure</strong>:\n<ul>\n<li>Optimization targeting only accuracy, not fairness,</li>\n<li>Equal weighting of all instances (ignoring group-specific harms),</li>\n<li>Ranking / decoding strategies that favor certain kinds of content.</li>\n</ul>\n</li>\n<li><strong>Evaluation</strong>:\n<ul>\n<li>Benchmarks not representative of real user populations,</li>\n<li>Metrics that ignore group disparities (e.g., reporting only overall accuracy).</li>\n</ul>\n</li>\n<li><strong>Deployment</strong>:\n<ul>\n<li>Using models in unanticipated contexts,</li>\n<li>User interfaces that over-trust model output,</li>\n<li>Removal (or presence) of human oversight.<br />\nRecognizing these stages helps locate where to intervene.</li>\n</ul>\n</li>\n</ol>\n<hr xmlns=\"http://www.w3.org/1999/xhtml\" />\n<h2 xmlns=\"http://www.w3.org/1999/xhtml\">Taxonomy of Bias Evaluation Metrics</h2>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">The authors propose a taxonomy based on <em>what part of the model or output</em> a metric uses [3, Table 3]:</p>\n<ol xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>Embedding-Based Metrics</strong> (use vector representations) [3.3]</li>\n<li><strong>Probability-Based Metrics</strong> (use token probabilities) [3.4]</li>\n<li><strong>Generated Text-Based Metrics</strong> (analyze free-text outputs) [3.5]<br />\nThey emphasize that metrics and datasets are often conflated: the same dataset can support multiple metrics, and the same metric can be applied to multiple datasets if the structures match [3.1, Figure 2].</li>\n</ol>\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">1. Embedding-Based Metrics</h3>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">These compare how close different words or sentences are in embedding space [3.3].</p>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>WEAT</strong> (Word Embedding Association Test) and extensions:\n<ul>\n<li>Measures how much, say, gendered words (he, she) are more similar to certain neutral concepts (e.g., “engineer” vs. “nurse”) [3.3.1].</li>\n<li>Uses cosine similarity and effect sizes to summarize bias.</li>\n</ul>\n</li>\n<li><strong>SEAT / CEAT</strong>:\n<ul>\n<li>Extend WEAT from static word embeddings to contextual sentence embeddings, using template sentences like “This is a [doctor]” [3.3.2].</li>\n<li>CEAT samples many contexts and aggregates effect sizes.</li>\n</ul>\n</li>\n<li><strong>Sentence Bias Score</strong>:\n<ul>\n<li>Computes how each word in a sentence aligns with a learned “gender direction” in embedding space, weighted by word importance, and sums these contributions [3.3.2].<br />\n<strong>Limitations:</strong> Empirical studies show weak or inconsistent links between embedding bias and actual downstream disparate outcomes; debiasing embeddings may just “hide” bias rather than remove it [3.3.3]. The survey recommends not relying solely on embedding-based metrics.</li>\n</ul>\n</li>\n</ul>\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">2. Probability-Based Metrics</h3>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">These use model-assigned probabilities for words or sentences [3.4, Figure 4].</p>\n<h4 xmlns=\"http://www.w3.org/1999/xhtml\">Masked-Token Methods</h4>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">Given templates like “The engineer said [MASK] would help,” models fill blanks; bias is inferred from how often different pronouns or descriptors are chosen:</p>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>DisCo</strong>: compares how often different group words lead to different completions [3.4.1].</li>\n<li><strong>Log-Probability Bias Score (LPBS)</strong>: normalizes conditional probabilities of group words (e.g., “man,” “woman”) by their overall priors, isolating bias attributable to the neutral context (e.g., profession) [3.4.1].</li>\n<li><strong>Categorical Bias Score</strong>: generalizes LPBS to more than two group categories, measuring variance across them [3.4.1].</li>\n</ul>\n<h4 xmlns=\"http://www.w3.org/1999/xhtml\">Pseudo-Log-Likelihood Methods</h4>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">These approximate how likely a full sentence is by summing log probabilities of each token [3.4.2]:</p>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>CrowS-Pairs Score</strong>: compares pseudo-log-likelihood of a stereotype sentence vs. a counterfactual, less stereotyped sentence [3.4.2].</li>\n<li><strong>StereoSet / CAT / iCAT</strong>: for each context, models choose between a stereotypical completion, an “anti-stereotype,” and a meaningless option; ideal behavior prefers meaningful over meaningless and selects stereotypes and anti-stereotypes equally often [3.4.2].</li>\n<li><strong>All Unmasked Likelihood (AUL/AULA)</strong>: considers full sentences without masking, optionally weighting tokens by attention scores [3.4.2].</li>\n<li><strong>Language Model Bias</strong>: uses differences in perplexity (a standard language-modeling measure) between biased and counterfactual sentences [3.4.2].<br />\n<strong>Limitations:</strong> As with embeddings, these metrics often correlate only weakly with real downstream harms. They also rely on narrow templates and simplistic assumptions (e.g., that stereotype vs. anti-stereotype choices should be exactly balanced), which may not reflect real-world power dynamics [3.4.3].</li>\n</ul>\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">3. Generated Text-Based Metrics</h3>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">These assess bias in free-text generations produced from prompts [3.5, Figure 5]. They are particularly useful when you only have black-box access to an API.</p>\n<h4 xmlns=\"http://www.w3.org/1999/xhtml\">Distribution-Based Metrics</h4>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">Look at how often certain words or patterns occur:</p>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>Social Group Substitution</strong>: check whether outputs stay (approximately) the same when only the group term in the prompt is changed [3.5.1].</li>\n<li><strong>Co-Occurrence Bias Score</strong>: measures how frequently a word appears near different group terms, comparing those conditional distributions [3.5.1].</li>\n<li><strong>Demographic Representation</strong>: counts how often group mentions appear, and compares the distribution to a reference (e.g., uniform or dataset-based) [3.5.1].</li>\n<li><strong>Stereotypical Associations</strong>: counts how often group terms co-occur with particular words that may signal stereotypes [3.5.1].</li>\n<li><strong>Markedness / Marked Persons</strong>: compares language used about “marked” (marginalized) identities vs. unmarked defaults to detect stereotypical framing [3.5.1].</li>\n</ul>\n<h4 xmlns=\"http://www.w3.org/1999/xhtml\">Classifier-Based Metrics</h4>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">Use an auxiliary classifier (e.g., for toxicity or sentiment):</p>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>Perspective API</strong> is often used to score toxicity [3.5.2].\n<ul>\n<li><strong>Expected Maximum Toxicity</strong>: highest toxicity across multiple generations [3.5.2].</li>\n<li><strong>Toxicity Probability</strong>: probability at least one toxic generation occurs [3.5.2].</li>\n<li><strong>Toxicity Fraction</strong>: average fraction of generations judged toxic [3.5.2].</li>\n</ul>\n</li>\n<li><strong>Score Parity</strong>: difference in average classifier score (e.g., sentiment, toxicity) across groups for counterfactual prompts [3.5.2].</li>\n<li><strong>Counterfactual Sentiment Bias</strong>: distance between sentiment distributions for different groups using Wasserstein distance [3.5.2].</li>\n<li><strong>Regard Score</strong>: uses a custom classifier to rate positive/negative “regard” (social connotation) towards groups [3.5.2].</li>\n<li><strong>Full Gen Bias</strong>: measures variance in style classification across prompts for different group terms [3.5.2].</li>\n<li>Other specialized classifiers (e.g., for heteronormativity, stereotyping of LGBTQ+ people) can also be plugged in [3.5.2].</li>\n</ul>\n<h4 xmlns=\"http://www.w3.org/1999/xhtml\">Lexicon-Based Metrics</h4>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">Compare each output word against precompiled lexicons:</p>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>HONEST</strong>: counts how often completions contain words from the HurtLex list of hurtful expressions [3.5.3].</li>\n<li><strong>Psycholinguistic Norms</strong>: aggregates expert-assigned “affective” scores (e.g., dominance, fear, sadness) for all words in generated text [3.5.3].</li>\n<li><strong>Gender Polarity</strong>: aggregates word-level gender-bias scores, including indirect gender markers, to obtain an overall genderedness measure of the text [3.5.3].<br />\n<strong>Limitations:</strong> These metrics are sensitive to decoding parameters (e.g., temperature, generation length); different settings can yield opposite conclusions about a model’s bias [3.5.4]. Classifier-based metrics inherit any bias in the classifier itself (e.g., toxicity detectors overflag African-American English). Lexicon methods are coarse and miss harmful patterns that span multiple words or depend on context [3.5.4].</li>\n</ul>\n<hr xmlns=\"http://www.w3.org/1999/xhtml\" />\n<h2 xmlns=\"http://www.w3.org/1999/xhtml\">Taxonomy of Bias Evaluation Datasets</h2>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">The survey also organizes datasets used to test LLM bias, focusing on <strong>English LLMs</strong> (plus some multilingual and machine translation contexts) [4]. Datasets are grouped by structure [4, Table 4]:</p>\n<ol xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>Counterfactual Inputs</strong> – pairs (or tuples) of sentences differing only in group terms [4.1].</li>\n<li><strong>Prompts</strong> – phrases for generative LLMs to complete or answer [4.2].</li>\n</ol>\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">1. Counterfactual Inputs</h3>\n<h4 xmlns=\"http://www.w3.org/1999/xhtml\">Masked-Token Datasets (for fill-in-the-blank tasks)</h4>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>Winogender, WinoBias, WinoBias+</strong>:\n<ul>\n<li>Coreference resolution templates testing gender–occupation stereotypes (engineer, nurse, etc.), often using sentences like “The engineer informed the client that [he/she/they] …” [4.1.1].</li>\n<li>Measure whether models associate certain professions disproportionately with one gender and whether performance differs for male vs. female pronouns.</li>\n</ul>\n</li>\n<li><strong>GAP / GAP-Subjective</strong>:\n<ul>\n<li>Realistic Wikipedia-based ambiguous pronoun-name pairs; assess gender bias in pronoun resolution [4.1.1].</li>\n<li>GAP-Subjective adds subjective style (opinion-bearing language) [4.1.1].</li>\n</ul>\n</li>\n<li><strong>BUG</strong>:\n<ul>\n<li>Large set of syntactically diverse coreference templates (over 100k) annotated as stereotypical or anti-stereotypical gender role assignments [4.1.1].</li>\n</ul>\n</li>\n<li><strong>StereoSet</strong>:\n<ul>\n<li>Crowdsourced short contexts with three options: stereotype, anti-stereotype, and meaningless; covers race, gender, religion, and profession [4.1.1].</li>\n<li>Used with CAT / iCAT metrics.</li>\n</ul>\n</li>\n<li><strong>BEC-Pro</strong>:\n<ul>\n<li>Profession-related templates with gendered person terms; tests gender biases in occupations [4.1.1].</li>\n</ul>\n</li>\n</ul>\n<h4 xmlns=\"http://www.w3.org/1999/xhtml\">Unmasked Sentence Datasets</h4>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">These provide full sentence pairs where a social group reference is swapped; models are asked to rate which sentence is more probable or to generate continuations [4.1.2]. Examples (from the table) include:</p>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>CrowS-Pairs</strong>: stereotype vs. anti-/less-stereotype pairs covering multiple axes (race, gender, sexual orientation, religion, etc.) [4, Table 4].</li>\n<li>Datasets like <strong>RedditBias</strong>, <strong>Bias-STS-B</strong>, <strong>PANDA</strong>, <strong>Equity Evaluation Corpus</strong>, and <strong>Bias NLI</strong> target different tasks (NLI, sentiment similarity, etc.) and different axes (e.g., race, gender, disability) [4, Table 4].<br />\nThese datasets enable probability-based metrics (e.g., pseudo-log-likelihood, perplexity) and generated-text comparisons across counterfactuals.</li>\n</ul>\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">2. Prompt-Based Datasets</h3>\n<h4 xmlns=\"http://www.w3.org/1999/xhtml\">Sentence Completion Prompts</h4>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">Used to elicit model continuation and then score resulting text [4.2.1]:</p>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>RealToxicityPrompts</strong>:\n<ul>\n<li>Prompts designed to elicit potentially toxic completions; used widely to benchmark toxicity and to test mitigation strategies [4, Table 4].</li>\n</ul>\n</li>\n<li><strong>BOLD</strong>:\n<ul>\n<li>Prompts across themes like gender, profession, race, religion, etc., constructed to examine representational harms and toxicity [4, Table 4].</li>\n</ul>\n</li>\n<li><strong>HolisticBias</strong>:\n<ul>\n<li>A large prompt set covering many identity dimensions (gender, race, disability, age, physical appearance, and more) [4, Table 4].</li>\n</ul>\n</li>\n<li><strong>HONEST</strong>:\n<ul>\n<li>Templates for testing derogatory completions about various identity terms, tied to the HONEST metric [4, Table 4].</li>\n</ul>\n</li>\n<li><strong>TrustGPT</strong>:\n<ul>\n<li>A small set of templates focused on trust-related behaviors and biases across groups [4, Table 4].</li>\n</ul>\n</li>\n</ul>\n<h4 xmlns=\"http://www.w3.org/1999/xhtml\">Question-Answering Prompts</h4>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">These pose explicit questions and candidate answers, allowing bias analysis in QA behavior [4.2.2]:</p>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>BBQ (Bias Benchmark for QA)</strong>:\n<ul>\n<li>Question-answering benchmark with ambiguous and disambiguated questions designed to detect whether models resolve ambiguity using stereotypes; covers numerous social dimensions (race, gender, disability, age, religion, etc.) [4, Table 4].</li>\n</ul>\n</li>\n<li><strong>UnQover</strong>:\n<ul>\n<li>QA templates focusing on occupation, crime, or other attributes across different groups, used to expose stereotype-based answers [4, Table 4].</li>\n</ul>\n</li>\n<li><strong>Grep-BiasIR</strong>:\n<ul>\n<li>Targets bias in information retrieval / QA contexts, analyzing how queries about different groups produce different answers [4, Table 4].<br />\nThe authors provide a GitHub repository consolidating many of these datasets for easier access [4].</li>\n</ul>\n</li>\n</ul>\n<hr xmlns=\"http://www.w3.org/1999/xhtml\" />\n<h2 xmlns=\"http://www.w3.org/1999/xhtml\">Taxonomy of Bias Mitigation Techniques</h2>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">Mitigation strategies are categorized by <em>where</em> they intervene in the LLM pipeline [2.4.3, §5]:</p>\n<ol xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>Pre-Processing</strong> – change inputs to the model (training data or prompts) [5.1].</li>\n<li><strong>In-Training</strong> – modify how the model is trained or fine-tuned [5.2].</li>\n<li><strong>Intra-Processing</strong> – alter inference-time behavior without retraining [5.3].</li>\n<li><strong>Post-Processing</strong> – edit model outputs after generation [5.4].</li>\n</ol>\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">1. Pre-Processing</h3>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>Data Augmentation</strong>: add counterfactual examples (e.g., swapping gender terms) to balance representations [5.1.1].</li>\n<li><strong>Data Filtering and Reweighting</strong>: remove or downweight biased or toxic instances; upweight underrepresented groups [5.1.2].</li>\n<li><strong>Data Generation</strong>: use models to generate synthetic but more balanced training data [5.1.3].</li>\n<li><strong>Instruction Tuning / Prompt Engineering</strong>: prepend instructions or safety policies to prompts so the model follows fairer behaviors [5.1.4].</li>\n<li><strong>Projection-Based Mitigation</strong>: transform internal representations via projection (e.g., removing gender directions) before feeding them into downstream components [5.1.5].</li>\n</ul>\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">2. In-Training</h3>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>Architecture Modification</strong>: change model components (e.g., attention mechanisms) to reduce bias leakage [5.2.1].</li>\n<li><strong>Loss Function Modification</strong>: add fairness-related regularizers or adversarial objectives so gradients penalize biased behavior [5.2.2].</li>\n<li><strong>Selective Parameter Updating</strong>: update only subsets of parameters during fine-tuning (e.g., adapters, LoRA layers) with bias-aware objectives [5.2.3].</li>\n<li><strong>Filtering Model Parameters</strong>: explicitly zero or remove parameters found to encode unwanted associations [5.2.4].</li>\n</ul>\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">3. Intra-Processing</h3>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>Decoding Strategy Modification</strong>: adjust token probabilities during generation (e.g., lower probabilities of toxic or slur tokens) [5.3.1].</li>\n<li><strong>Weight Redistribution</strong>: manipulate attention weights (e.g., to reduce emphasis on harmful tokens or contexts) [5.3.2].</li>\n<li><strong>Modular Debiasing Networks</strong>: add small, separately trained components that modulate the main model’s outputs during inference [5.3.3].</li>\n</ul>\n<h3 xmlns=\"http://www.w3.org/1999/xhtml\">4. Post-Processing</h3>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>Rewriting</strong>: detect harmful content in generated text and rewrite or filter it (e.g., redaction, paraphrasing with safer language) [5.4.1].<br />\nThe survey notes that no single intervention is sufficient: bias can persist or re-emerge at each stage, so multiple layers of mitigation are often necessary [2.4.3, 5].</li>\n</ul>\n<hr xmlns=\"http://www.w3.org/1999/xhtml\" />\n<h2 xmlns=\"http://www.w3.org/1999/xhtml\">Open Problems and Recommendations</h2>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">The authors highlight several cross-cutting challenges [2.4, 3.6, 4, 5]:</p>\n<ol xmlns=\"http://www.w3.org/1999/xhtml\">\n<li><strong>Mismatch between metrics and real-world harms</strong><br />\nEmbedding and probability-based metrics often correlate weakly with actual downstream, user-facing harms. Evaluations should focus more on <em>task-level</em> performance and clear harm definitions rather than abstract proxies [3.3.3, 3.4.3, 3.6].</li>\n<li><strong>Inadequate conceptualization of fairness</strong><br />\nMany works assume “bias” is undesirable without specifying <em>what</em> harm is involved and <em>why</em>. Researchers should name specific harms (e.g., stereotyping, toxicity, erasure), attend to power dynamics, and state normative assumptions (e.g., why a particular equality constraint is appropriate) [2.2.1, 2.3, 3.6].</li>\n<li><strong>Dataset quality and coverage</strong><br />\nMany benchmarks rely on narrow templates, limited groups, or ad hoc definitions of stereotypes/anti-stereotypes. Future work should:\n<ul>\n<li>Cover more languages and dialects,</li>\n<li>Better represent intersectional identities,</li>\n<li>Ensure reliability and validity of labels [2, 4].</li>\n</ul>\n</li>\n<li><strong>Bias in auxiliary tools</strong><br />\nToxicity and sentiment classifiers, as well as lexicons, can themselves be biased (e.g., mislabeling African-American English or disability-related language). Using them uncritically can misrepresent model harms [3.5.4].</li>\n<li><strong>Dynamic norms and context-sensitivity</strong><br />\nWhat counts as harmful or fair changes over time and varies across cultures and contexts. Static benchmarks and metrics may rapidly become outdated or misaligned with community perspectives [2.2.1, 2.3].</li>\n<li><strong>Power imbalances in LLM development</strong><br />\nControl over training data, model design, and deployment is concentrated among a few institutions. The paper calls for work that:\n<ul>\n<li>Involves affected communities in defining fairness goals,</li>\n<li>Increases transparency and accountability in LLM pipelines,</li>\n<li>Reduces asymmetries in who benefits and who bears the risks [5].</li>\n</ul>\n</li>\n</ol>\n<hr xmlns=\"http://www.w3.org/1999/xhtml\" />\n<h2 xmlns=\"http://www.w3.org/1999/xhtml\">Overall Significance</h2>\n<p xmlns=\"http://www.w3.org/1999/xhtml\">This survey does not present new experimental results on a single model; instead, it:</p>\n<ul xmlns=\"http://www.w3.org/1999/xhtml\">\n<li>Unifies fragmented literature on LLM bias into common <strong>definitions</strong>, <strong>taxonomies</strong>, and <strong>notation</strong>;</li>\n<li>Clarifies the relationship between <strong>metrics</strong>, <strong>datasets</strong>, and <strong>mitigation methods</strong>;</li>\n<li>Critically evaluates the strengths and weaknesses of existing approaches;</li>\n<li>Identifies structural and conceptual challenges that technical fixes alone cannot resolve [1–5].<br />\nFor practitioners and researchers, it serves as a guide to:</li>\n<li>Choose appropriate evaluation metrics and datasets for a given use case,</li>\n<li>Understand what kinds of harms a given metric/dataset can or cannot detect,</li>\n<li>Select and combine mitigation methods at different stages of the LLM pipeline,</li>\n<li>Recognize open research questions where more robust and theoretically grounded work is needed.</li>\n</ul>",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-28T04:04:27Z",
          "dateModified": "2025-11-28T04:04:27Z",
          "uri": "http://zotero.org/users/12031879/items/NYU4N6S2"
        }
      ],
      "citationKey": "gallegosBiasFairnessLarge2024",
      "itemKey": "TJEMXLJ4",
      "libraryID": 1,
      "select": "zotero://select/library/items/TJEMXLJ4"
    },
    {
      "key": "7C94DQVF",
      "version": 5090,
      "itemType": "conferencePaper",
      "title": "Gender trouble in language models: An empirical audit guided by gender performativity theory",
      "date": "2025",
      "extra": "Citation Key: hafner2025gendertrouble",
      "pages": "1677–1695",
      "proceedingsTitle": "Proceedings of the 2025 ACM conference on fairness, accountability, and transparency",
      "DOI": "10.1145/3715275.3732112",
      "creators": [
        {
          "firstName": "Florian Sofia",
          "lastName": "Hafner",
          "creatorType": "author"
        },
        {
          "firstName": "Ana",
          "lastName": "Valdivia",
          "creatorType": "author"
        },
        {
          "firstName": "Lucía",
          "lastName": "Rocher",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T04:17:17Z",
      "dateModified": "2025-11-23T04:17:17Z",
      "uri": "http://zotero.org/users/12031879/items/7C94DQVF",
      "itemID": 2217,
      "attachments": [],
      "notes": [],
      "citationKey": "hafner2025gendertrouble",
      "itemKey": "7C94DQVF",
      "libraryID": 1,
      "select": "zotero://select/library/items/7C94DQVF"
    },
    {
      "key": "TYBQHK4I",
      "version": 5151,
      "itemType": "journalArticle",
      "title": "Gender biases within Artificial Intelligence and ChatGPT: Evidence, Sources of Biases and Solutions",
      "date": "05/2025",
      "language": "en",
      "shortTitle": "Gender biases within Artificial Intelligence and ChatGPT",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://linkinghub.elsevier.com/retrieve/pii/S2949882125000295",
      "accessDate": "2025-11-25T01:18:10Z",
      "volume": "4",
      "pages": "100145",
      "publicationTitle": "Computers in Human Behavior: Artificial Humans",
      "DOI": "10.1016/j.chbah.2025.100145",
      "journalAbbreviation": "Computers in Human Behavior: Artificial Humans",
      "ISSN": "29498821",
      "creators": [
        {
          "firstName": "Jerlyn Q.H.",
          "lastName": "Ho",
          "creatorType": "author"
        },
        {
          "firstName": "Andree",
          "lastName": "Hartanto",
          "creatorType": "author"
        },
        {
          "firstName": "Andrew",
          "lastName": "Koh",
          "creatorType": "author"
        },
        {
          "firstName": "Nadyanna M.",
          "lastName": "Majeed",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-25T01:18:10Z",
      "dateModified": "2025-11-25T01:18:10Z",
      "uri": "http://zotero.org/users/12031879/items/TYBQHK4I",
      "itemID": 2250,
      "attachments": [],
      "notes": [],
      "citationKey": "hoGenderBiasesArtificial2025",
      "itemKey": "TYBQHK4I",
      "libraryID": 1,
      "select": "zotero://select/library/items/TYBQHK4I"
    },
    {
      "key": "LV3D38IT",
      "version": 5155,
      "itemType": "conferencePaper",
      "title": "MISGENDERED: Limits of Large Language Models in Understanding Pronouns",
      "date": "2023",
      "language": "en",
      "shortTitle": "MISGENDERED",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://aclanthology.org/2023.acl-long.293",
      "accessDate": "2025-11-26T23:21:38Z",
      "place": "Toronto, Canada",
      "publisher": "Association for Computational Linguistics",
      "pages": "5352-5367",
      "proceedingsTitle": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      "conferenceName": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      "DOI": "10.18653/v1/2023.acl-long.293",
      "creators": [
        {
          "firstName": "Tamanna",
          "lastName": "Hossain",
          "creatorType": "author"
        },
        {
          "firstName": "Sunipa",
          "lastName": "Dev",
          "creatorType": "author"
        },
        {
          "firstName": "Sameer",
          "lastName": "Singh",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-26T23:21:38Z",
      "dateModified": "2025-11-26T23:21:38Z",
      "uri": "http://zotero.org/users/12031879/items/LV3D38IT",
      "itemID": 2258,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Full Text",
          "url": "https://aclanthology.org/2023.acl-long.293.pdf",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-26T23:21:39Z",
          "dateModified": "2025-11-26T23:21:39Z",
          "uri": "http://zotero.org/users/12031879/items/AN3JICIV",
          "path": "C:\\Users\\lilyc\\Zotero\\storage\\AN3JICIV\\Hossain et al. - 2023 - MISGENDERED Limits of Large Language Models in Understanding Pronouns.pdf",
          "select": "zotero://select/library/items/AN3JICIV"
        }
      ],
      "notes": [],
      "citationKey": "hossainMISGENDEREDLimitsLarge2023",
      "itemKey": "LV3D38IT",
      "libraryID": 1,
      "select": "zotero://select/library/items/LV3D38IT"
    },
    {
      "key": "SS7S2ZR9",
      "version": 5025,
      "itemType": "document",
      "title": "Unveiling gender bias in large language models: Using teacher's evaluation in higher education as an example",
      "date": "2024",
      "url": "https://arxiv.org/abs/2409.09652",
      "extra": "Citation Key: huang_unveiling_2024\nDOI: 10.48550/arXiv.2409.09652\ntex.howpublished: arXiv preprint",
      "creators": [
        {
          "firstName": "Yuanning",
          "lastName": "Huang",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T03:44:20Z",
      "dateModified": "2025-11-23T03:44:20Z",
      "uri": "http://zotero.org/users/12031879/items/SS7S2ZR9",
      "itemID": 2175,
      "attachments": [],
      "notes": [],
      "citationKey": "huang_unveiling_2024",
      "itemKey": "SS7S2ZR9",
      "libraryID": 1,
      "select": "zotero://select/library/items/SS7S2ZR9"
    },
    {
      "key": "9BBJ9LA2",
      "version": 5159,
      "itemType": "preprint",
      "title": "Mitigating Bias in Queer Representation within Large Language Models: A Collaborative Agent Approach",
      "abstractNote": "Large Language Models (LLMs) often perpetuate biases in pronoun usage, leading to misrepresentation or exclusion of queer individuals. This paper addresses the specific problem of biased pronoun usage in LLM outputs, particularly the inappropriate use of traditionally gendered pronouns (\"he,\" \"she\") when inclusive language is needed to accurately represent all identities. We introduce a collaborative agent pipeline designed to mitigate these biases by analyzing and optimizing pronoun usage for inclusivity. Our multi-agent framework includes specialized agents for both bias detection and correction. Experimental evaluations using the Tango dataset-a benchmark focused on gender pronoun usage-demonstrate that our approach significantly improves inclusive pronoun classification, achieving a 32.6 percentage point increase over GPT-4o in correctly disagreeing with inappropriate traditionally gendered pronouns $(χ^2 = 38.57, p &lt; 0.0001)$. These results accentuate the potential of agent-driven frameworks in enhancing fairness and inclusivity in AI-generated content, demonstrating their efficacy in reducing biases and promoting socially responsible AI.",
      "date": "2024",
      "shortTitle": "Mitigating Bias in Queer Representation within Large Language Models",
      "libraryCatalog": "DOI.org (Datacite)",
      "url": "https://arxiv.org/abs/2411.07656",
      "accessDate": "2025-11-26T23:22:43Z",
      "rights": "Creative Commons Attribution Share Alike 4.0 International",
      "extra": "Version Number: 2",
      "DOI": "10.48550/ARXIV.2411.07656",
      "repository": "arXiv",
      "creators": [
        {
          "firstName": "Tianyi",
          "lastName": "Huang",
          "creatorType": "author"
        },
        {
          "firstName": "Arya",
          "lastName": "Somasundaram",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computation and Language (cs.CL)",
          "type": 1
        },
        {
          "tag": "FOS: Computer and information sciences",
          "type": 1
        },
        {
          "tag": "Multiagent Systems (cs.MA)",
          "type": 1
        }
      ],
      "relations": {},
      "dateAdded": "2025-11-26T23:22:43Z",
      "dateModified": "2025-11-26T23:22:43Z",
      "uri": "http://zotero.org/users/12031879/items/9BBJ9LA2",
      "itemID": 2262,
      "attachments": [],
      "notes": [
        {
          "key": "UKS4NC8U",
          "version": 5159,
          "itemType": "note",
          "parentItem": "9BBJ9LA2",
          "note": "<h2>Other</h2>\nNeurIPS 2024 Queer in AI Workshop",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-26T23:22:43Z",
          "dateModified": "2025-11-26T23:22:43Z",
          "uri": "http://zotero.org/users/12031879/items/UKS4NC8U"
        }
      ],
      "citationKey": "huangMitigatingBiasQueer2024",
      "itemKey": "9BBJ9LA2",
      "libraryID": 1,
      "select": "zotero://select/library/items/9BBJ9LA2"
    },
    {
      "key": "2FZKQRQU",
      "version": 5046,
      "itemType": "conferencePaper",
      "title": "Gender bias in masked language models for multiple languages",
      "abstractNote": "Masked Language Models (MLMs) pre-trained by predicting masked tokens on large corpora have been used successfully in natural language processing tasks for a variety of languages. Unfortunately, it was reported that MLMs also learn discriminative biases regarding attributes such as gender and race. Because most studies have focused on MLMs in English, the bias of MLMs in other languages has rarely been investigated. Manual annotation of evaluation data for languages other than English has been challenging due to the cost and difficulty in recruiting annotators. Moreover, the existing bias evaluation methods require the stereotypical sentence pairs consisting of the same context with attribute words (e.g. He/She is a nurse).We propose Multilingual Bias Evaluation (MBE) score, to evaluate bias in various languages using only English attribute word lists and parallel corpora between the target language and English without requiring manually annotated data. We evaluated MLMs in eight languages using the MBE and confirmed that gender-related biases are encoded in MLMs for all those languages. We manually created datasets for gender bias in Japanese and Russian to evaluate the validity of the MBE.The results show that the bias scores reported by the MBE significantly correlates with that computed from the above manually created datasets and the existing English datasets for gender bias.",
      "date": "2022-07",
      "url": "https://aclanthology.org/2022.naacl-main.197/",
      "extra": "Citation Key: kaneko-etal-2022-gender",
      "place": "Seattle, United States",
      "publisher": "Association for Computational Linguistics",
      "pages": "2740–2750",
      "proceedingsTitle": "Proceedings of the 2022 conference of the north american chapter of the association for computational linguistics: Human language technologies",
      "DOI": "10.18653/v1/2022.naacl-main.197",
      "creators": [
        {
          "firstName": "Masahiro",
          "lastName": "Kaneko",
          "creatorType": "author"
        },
        {
          "firstName": "Aizhan",
          "lastName": "Imankulova",
          "creatorType": "author"
        },
        {
          "firstName": "Danushka",
          "lastName": "Bollegala",
          "creatorType": "author"
        },
        {
          "firstName": "Naoaki",
          "lastName": "Okazaki",
          "creatorType": "author"
        },
        {
          "firstName": "Marine",
          "lastName": "Carpuat",
          "creatorType": "editor"
        },
        {
          "firstName": "Marie-Catherine",
          "lastName": "de Marneffe",
          "creatorType": "editor"
        },
        {
          "firstName": "Ivan Vladimir",
          "lastName": "Meza Ruiz",
          "creatorType": "editor"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T04:00:53Z",
      "dateModified": "2025-11-23T04:00:53Z",
      "uri": "http://zotero.org/users/12031879/items/2FZKQRQU",
      "itemID": 2187,
      "attachments": [],
      "notes": [],
      "citationKey": "kaneko-etal-2022-gender",
      "itemKey": "2FZKQRQU",
      "libraryID": 1,
      "select": "zotero://select/library/items/2FZKQRQU"
    },
    {
      "key": "7PT4J4GH",
      "version": 5151,
      "itemType": "journalArticle",
      "title": "What’s in a Name? Experimental Evidence of Gender Bias in Recommendation Letters Generated by ChatGPT",
      "abstractNote": "Background\n              Artificial intelligence chatbots such as ChatGPT (OpenAI) have garnered excitement about their potential for delegating writing tasks ordinarily performed by humans. Many of these tasks (eg, writing recommendation letters) have social and professional ramifications, making the potential social biases in ChatGPT’s underlying language model a serious concern.\n            \n            \n              Objective\n              Three preregistered studies used the text analysis program Linguistic Inquiry and Word Count to investigate gender bias in recommendation letters written by ChatGPT in human-use sessions (N=1400 total letters).\n            \n            \n              Methods\n              We conducted analyses using 22 existing Linguistic Inquiry and Word Count dictionaries, as well as 6 newly created dictionaries based on systematic reviews of gender bias in recommendation letters, to compare recommendation letters generated for the 200 most historically popular “male” and “female” names in the United States. Study 1 used 3 different letter-writing prompts intended to accentuate professional accomplishments associated with male stereotypes, female stereotypes, or neither. Study 2 examined whether lengthening each of the 3 prompts while holding the between-prompt word count constant modified the extent of bias. Study 3 examined the variability within letters generated for the same name and prompts. We hypothesized that when prompted with gender-stereotyped professional accomplishments, ChatGPT would evidence gender-based language differences replicating those found in systematic reviews of human-written recommendation letters (eg, more affiliative, social, and communal language for female names; more agentic and skill-based language for male names).\n            \n            \n              Results\n              Significant differences in language between letters generated for female versus male names were observed across all prompts, including the prompt hypothesized to be neutral, and across nearly all language categories tested. Historically female names received significantly more social referents (5/6, 83% of prompts), communal or doubt-raising language (4/6, 67% of prompts), personal pronouns (4/6, 67% of prompts), and clout language (5/6, 83% of prompts). Contradicting the study hypotheses, some gender differences (eg, achievement language and agentic language) were significant in both the hypothesized and nonhypothesized directions, depending on the prompt. Heteroscedasticity between male and female names was observed in multiple linguistic categories, with greater variance for historically female names than for historically male names.\n            \n            \n              Conclusions\n              ChatGPT reproduces many gender-based language biases that have been reliably identified in investigations of human-written reference letters, although these differences vary across prompts and language categories. Caution should be taken when using ChatGPT for tasks that have social consequences, such as reference letter writing. The methods developed in this study may be useful for ongoing bias testing among progressive generations of chatbots across a range of real-world scenarios.\n            \n            \n              Trial Registration\n              OSF Registries osf.io/ztv96; https://osf.io/ztv96",
      "date": "2024-3-5",
      "language": "en",
      "shortTitle": "What’s in a Name?",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://www.jmir.org/2024/1/e51837",
      "accessDate": "2025-11-25T01:17:29Z",
      "volume": "26",
      "pages": "e51837",
      "publicationTitle": "Journal of Medical Internet Research",
      "DOI": "10.2196/51837",
      "journalAbbreviation": "J Med Internet Res",
      "ISSN": "1438-8871",
      "creators": [
        {
          "firstName": "Deanna M",
          "lastName": "Kaplan",
          "creatorType": "author"
        },
        {
          "firstName": "Roman",
          "lastName": "Palitsky",
          "creatorType": "author"
        },
        {
          "firstName": "Santiago J",
          "lastName": "Arconada Alvarez",
          "creatorType": "author"
        },
        {
          "firstName": "Nicole S",
          "lastName": "Pozzo",
          "creatorType": "author"
        },
        {
          "firstName": "Morgan N",
          "lastName": "Greenleaf",
          "creatorType": "author"
        },
        {
          "firstName": "Ciara A",
          "lastName": "Atkinson",
          "creatorType": "author"
        },
        {
          "firstName": "Wilbur A",
          "lastName": "Lam",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-25T01:17:29Z",
      "dateModified": "2025-11-25T01:17:29Z",
      "uri": "http://zotero.org/users/12031879/items/7PT4J4GH",
      "itemID": 2247,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Full Text",
          "url": "https://www.jmir.org/2024/1/e51837/PDF",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-25T01:17:31Z",
          "dateModified": "2025-11-25T01:17:31Z",
          "uri": "http://zotero.org/users/12031879/items/NZTWYA3Z",
          "path": "C:\\Users\\lilyc\\Zotero\\storage\\NZTWYA3Z\\Kaplan et al. - 2024 - What’s in a Name Experimental Evidence of Gender Bias in Recommendation Letters Generated by ChatGP.pdf",
          "select": "zotero://select/library/items/NZTWYA3Z"
        }
      ],
      "notes": [],
      "citationKey": "kaplanWhatsNameExperimental2024",
      "itemKey": "7PT4J4GH",
      "libraryID": 1,
      "select": "zotero://select/library/items/7PT4J4GH"
    },
    {
      "key": "4RSEVG5D",
      "version": 5024,
      "itemType": "report",
      "title": "Language models and cognitive automation for economic research",
      "date": "2023",
      "url": "http://www.nber.org/papers/w30957",
      "extra": "Citation Key: korinek_language_2023\nDOI: 10.3386/w30957",
      "place": "Cambridge, MA",
      "reportNumber": "w30957",
      "institution": "National Bureau of Economic Research",
      "creators": [
        {
          "firstName": "Anton",
          "lastName": "Korinek",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T03:44:20Z",
      "dateModified": "2025-11-23T03:44:20Z",
      "uri": "http://zotero.org/users/12031879/items/4RSEVG5D",
      "itemID": 2170,
      "attachments": [],
      "notes": [],
      "citationKey": "korinek_language_2023",
      "itemKey": "4RSEVG5D",
      "libraryID": 1,
      "select": "zotero://select/library/items/4RSEVG5D"
    },
    {
      "key": "SK95RTZM",
      "version": 5024,
      "itemType": "conferencePaper",
      "title": "Gender bias and stereotypes in large language models",
      "date": "2023",
      "url": "https://doi.org/10.1145/3582269.3615599",
      "extra": "Citation Key: kotek_gender_2023",
      "place": "New York, NY, USA",
      "publisher": "Association for Computing Machinery",
      "pages": "12–24",
      "proceedingsTitle": "Proceedings of the ACM collective intelligence conference",
      "DOI": "10.1145/3582269.3615599",
      "creators": [
        {
          "firstName": "Hadas",
          "lastName": "Kotek",
          "creatorType": "author"
        },
        {
          "firstName": "Rikker",
          "lastName": "Dockum",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Sun",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T03:44:20Z",
      "dateModified": "2025-11-23T03:44:20Z",
      "uri": "http://zotero.org/users/12031879/items/SK95RTZM",
      "itemID": 2168,
      "attachments": [],
      "notes": [],
      "citationKey": "kotek_gender_2023",
      "itemKey": "SK95RTZM",
      "libraryID": 1,
      "select": "zotero://select/library/items/SK95RTZM"
    },
    {
      "key": "G3FMDBRL",
      "version": 5110,
      "itemType": "conferencePaper",
      "title": "McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models",
      "date": "2025",
      "language": "en",
      "shortTitle": "McBE",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://aclanthology.org/2025.findings-acl.313",
      "accessDate": "2025-11-23T04:20:19Z",
      "place": "Vienna, Austria",
      "publisher": "Association for Computational Linguistics",
      "pages": "6033-6056",
      "proceedingsTitle": "Findings of the Association for Computational Linguistics: ACL 2025",
      "conferenceName": "Findings of the Association for Computational Linguistics: ACL 2025",
      "DOI": "10.18653/v1/2025.findings-acl.313",
      "creators": [
        {
          "firstName": "Tian",
          "lastName": "Lan",
          "creatorType": "author"
        },
        {
          "firstName": "Xiangdong",
          "lastName": "Su",
          "creatorType": "author"
        },
        {
          "firstName": "Xu",
          "lastName": "Liu",
          "creatorType": "author"
        },
        {
          "firstName": "Ruirui",
          "lastName": "Wang",
          "creatorType": "author"
        },
        {
          "firstName": "Ke",
          "lastName": "Chang",
          "creatorType": "author"
        },
        {
          "firstName": "Jiang",
          "lastName": "Li",
          "creatorType": "author"
        },
        {
          "firstName": "Guanglai",
          "lastName": "Gao",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T04:20:19Z",
      "dateModified": "2025-11-23T04:20:19Z",
      "uri": "http://zotero.org/users/12031879/items/G3FMDBRL",
      "itemID": 2230,
      "attachments": [],
      "notes": [],
      "citationKey": "lanMcBEMultitaskChinese2025",
      "itemKey": "G3FMDBRL",
      "libraryID": 1,
      "select": "zotero://select/library/items/G3FMDBRL"
    },
    {
      "key": "7ZLSSBVM",
      "version": 5110,
      "itemType": "journalArticle",
      "title": "The life cycle of large language models in education: A framework for understanding sources of bias",
      "abstractNote": "Abstract\n              Large language models (LLMs) are increasingly adopted in educational contexts to provide personalized support to students and teachers. The unprecedented capacity of LLM‐based applications to understand and generate natural language can potentially improve instructional effectiveness and learning outcomes, but the integration of LLMs in education technology has renewed concerns over algorithmic bias, which may exacerbate educational inequalities. Building on prior work that mapped the traditional machine learning life cycle, we provide a framework of the LLM life cycle from the initial development of LLMs to customizing pre‐trained models for various applications in educational settings. We explain each step in the LLM life cycle and identify potential sources of bias that may arise in the context of education. We discuss why current measures of bias from traditional machine learning fail to transfer to LLM‐generated text (eg, tutoring conversations) because text encodings are high‐dimensional, there can be multiple correct responses, and tailoring responses may be pedagogically desirable rather than unfair. The proposed framework clarifies the complex nature of bias in LLM applications and provides practical guidance for their evaluation to promote educational equity.\n            \n            \n              \n              \n                \n                  \n                    Practitioner notes\n                  \n                  \n                    What is already known about this topic\n                    \n                      \n                        The life cycle of traditional machine learning (ML) applications which focus on predicting labels is well understood.\n                      \n                      \n                        Biases are known to enter in traditional ML applications at various points in the life cycle, and methods to measure and mitigate these biases have been developed and tested.\n                      \n                      \n                        Large language models (LLMs) and other forms of generative artificial intelligence (GenAI) are increasingly adopted in education technologies (EdTech), but current evaluation approaches are not specific to the domain of education.\n                      \n                    \n                  \n                  \n                    What this paper adds\n                    \n                      \n                        A holistic perspective of the LLM life cycle with domain‐specific examples in education to highlight opportunities and challenges for incorporating natural language understanding (NLU) and natural language generation (NLG) into EdTech.\n                      \n                      \n                        Potential sources of bias are identified in each step of the LLM life cycle and discussed in the context of education.\n                      \n                      \n                        A framework for understanding where to expect potential harms of LLMs for students, teachers, and other users of GenAI technology in education, which can guide approaches to bias measurement and mitigation.\n                      \n                    \n                  \n                  \n                    Implications for practice and/or policy\n                    \n                      \n                        Education practitioners and policymakers should be aware that biases can originate from a multitude of steps in the LLM life cycle, and the life cycle perspective offers them a heuristic for asking technology developers to explain each step to assess the risk of bias.\n                      \n                      \n                        Measuring the biases of systems that use LLMs in education is more complex than with traditional ML, in large part because the evaluation of natural language generation is highly context‐dependent (eg, what counts as good feedback on an assignment varies).\n                      \n                      \n                        EdTech developers can play an important role in collecting and curating datasets for the evaluation and benchmarking of LLM applications moving forward.",
      "date": "09/2024",
      "language": "en",
      "shortTitle": "The life cycle of large language models in education",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13505",
      "accessDate": "2025-11-23T04:19:21Z",
      "volume": "55",
      "pages": "1982-2002",
      "publicationTitle": "British Journal of Educational Technology",
      "DOI": "10.1111/bjet.13505",
      "issue": "5",
      "journalAbbreviation": "Brit J Educational Tech",
      "ISSN": "0007-1013, 1467-8535",
      "creators": [
        {
          "firstName": "Jinsook",
          "lastName": "Lee",
          "creatorType": "author"
        },
        {
          "firstName": "Yann",
          "lastName": "Hicke",
          "creatorType": "author"
        },
        {
          "firstName": "Renzhe",
          "lastName": "Yu",
          "creatorType": "author"
        },
        {
          "firstName": "Christopher",
          "lastName": "Brooks",
          "creatorType": "author"
        },
        {
          "firstName": "René F.",
          "lastName": "Kizilcec",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T04:19:21Z",
      "dateModified": "2025-11-23T04:19:21Z",
      "uri": "http://zotero.org/users/12031879/items/7ZLSSBVM",
      "itemID": 2224,
      "attachments": [],
      "notes": [],
      "citationKey": "leeLifeCycleLarge2024",
      "itemKey": "7ZLSSBVM",
      "libraryID": 1,
      "select": "zotero://select/library/items/7ZLSSBVM"
    },
    {
      "key": "ZWUA5P9Y",
      "version": 5090,
      "itemType": "document",
      "title": "Dual debiasing: Remove stereotypes and keep factual gender for fair language modeling and translation",
      "date": "2025",
      "extra": "Citation Key: limisiewicz2025dual\narXiv: 2501.10150 [cs.CL]\nDOI: 10.48550/arXiv.2501.10150",
      "creators": [
        {
          "firstName": "Tomasz",
          "lastName": "Limisiewicz",
          "creatorType": "author"
        },
        {
          "firstName": "David",
          "lastName": "Mareček",
          "creatorType": "author"
        },
        {
          "firstName": "Tomáš",
          "lastName": "Musil",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T04:17:17Z",
      "dateModified": "2025-11-23T04:17:17Z",
      "uri": "http://zotero.org/users/12031879/items/ZWUA5P9Y",
      "itemID": 2218,
      "attachments": [],
      "notes": [],
      "citationKey": "limisiewicz2025dual",
      "itemKey": "ZWUA5P9Y",
      "libraryID": 1,
      "select": "zotero://select/library/items/ZWUA5P9Y"
    },
    {
      "key": "CVKHS7YC",
      "version": 5151,
      "itemType": "journalArticle",
      "title": "Computer says ‘no’: Exploring systemic bias in ChatGPT using an audit approach",
      "date": "01/2024",
      "language": "en",
      "shortTitle": "Computer says ‘no’",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://linkinghub.elsevier.com/retrieve/pii/S2949882124000148",
      "accessDate": "2025-11-25T01:18:20Z",
      "volume": "2",
      "pages": "100054",
      "publicationTitle": "Computers in Human Behavior: Artificial Humans",
      "DOI": "10.1016/j.chbah.2024.100054",
      "issue": "1",
      "journalAbbreviation": "Computers in Human Behavior: Artificial Humans",
      "ISSN": "29498821",
      "creators": [
        {
          "firstName": "Louis",
          "lastName": "Lippens",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-25T01:18:20Z",
      "dateModified": "2025-11-25T01:18:20Z",
      "uri": "http://zotero.org/users/12031879/items/CVKHS7YC",
      "itemID": 2251,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Full Text",
          "url": "https://biblio.ugent.be/publication/01HQJV82M4XY2PDVB0BTT257EP/file/01HQJV9V3QRMG39Y8A9CY7MDEQ.pdf",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-25T01:19:01Z",
          "dateModified": "2025-11-25T01:19:01Z",
          "uri": "http://zotero.org/users/12031879/items/BCC39V63",
          "path": "C:\\Users\\lilyc\\Zotero\\storage\\BCC39V63\\Lippens - 2024 - Computer says ‘no’ Exploring systemic bias in ChatGPT using an audit approach.pdf",
          "select": "zotero://select/library/items/BCC39V63"
        }
      ],
      "notes": [],
      "citationKey": "lippensComputerSaysNo2024",
      "itemKey": "CVKHS7YC",
      "libraryID": 1,
      "select": "zotero://select/library/items/CVKHS7YC"
    },
    {
      "key": "KZ68W68J",
      "version": 5038,
      "itemType": "preprint",
      "title": "Gender Bias in Neural Natural Language Processing",
      "abstractNote": "We examine whether neural natural language processing (NLP) systems reflect historical biases in training data. We define a general benchmark to quantify gender bias in a variety of neural NLP tasks. Our empirical evaluation with state-of-the-art neural coreference resolution and textbook RNN-based language models trained on benchmark datasets finds significant gender bias in how models view occupations. We then mitigate bias with CDA: a generic methodology for corpus augmentation via causal interventions that breaks associations between gendered and gender-neutral words. We empirically show that CDA effectively decreases gender bias while preserving accuracy. We also explore the space of mitigation strategies with CDA, a prior approach to word embedding debiasing (WED), and their compositions. We show that CDA outperforms WED, drastically so when word embeddings are trained. For pre-trained embeddings, the two methods can be effectively composed. We also find that as training proceeds on the original data set with gradient descent the gender bias grows as the loss reduces, indicating that the optimization encourages bias; CDA mitigates this behavior.",
      "date": "2019-05-30",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/1807.11714",
      "accessDate": "2025-11-23T03:49:29Z",
      "extra": "arXiv:1807.11714 [cs]",
      "DOI": "10.48550/arXiv.1807.11714",
      "repository": "arXiv",
      "archiveID": "arXiv:1807.11714",
      "creators": [
        {
          "firstName": "Kaiji",
          "lastName": "Lu",
          "creatorType": "author"
        },
        {
          "firstName": "Piotr",
          "lastName": "Mardziel",
          "creatorType": "author"
        },
        {
          "firstName": "Fangjing",
          "lastName": "Wu",
          "creatorType": "author"
        },
        {
          "firstName": "Preetam",
          "lastName": "Amancharla",
          "creatorType": "author"
        },
        {
          "firstName": "Anupam",
          "lastName": "Datta",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Computation and Language",
          "type": 1
        }
      ],
      "relations": {},
      "dateAdded": "2025-11-23T03:49:29Z",
      "dateModified": "2025-11-23T03:49:29Z",
      "uri": "http://zotero.org/users/12031879/items/KZ68W68J",
      "itemID": 2177,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Preprint PDF",
          "url": "http://arxiv.org/pdf/1807.11714v2",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-23T03:49:29Z",
          "dateModified": "2025-11-23T03:49:29Z",
          "uri": "http://zotero.org/users/12031879/items/FJFM6FVQ",
          "path": "C:\\Users\\lilyc\\Zotero\\storage\\FJFM6FVQ\\Lu et al. - 2019 - Gender Bias in Neural Natural Language Processing.pdf",
          "select": "zotero://select/library/items/FJFM6FVQ"
        },
        {
          "itemType": "attachment",
          "title": "Snapshot",
          "url": "https://arxiv.org/abs/1807.11714",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-23T03:49:34Z",
          "dateModified": "2025-11-23T03:49:34Z",
          "uri": "http://zotero.org/users/12031879/items/WKIPL8I7",
          "path": "C:\\Users\\lilyc\\Zotero\\storage\\WKIPL8I7\\1807.html",
          "select": "zotero://select/library/items/WKIPL8I7"
        }
      ],
      "notes": [],
      "citationKey": "luGenderBiasNeural2019",
      "itemKey": "KZ68W68J",
      "libraryID": 1,
      "select": "zotero://select/library/items/KZ68W68J"
    },
    {
      "key": "8PF8T4KB",
      "version": 5114,
      "itemType": "journalArticle",
      "title": "Quantifying gender bias in large language models using information-theoretic and statistical analysis",
      "date": "2025",
      "extra": "Citation Key: mirza2025quantifying",
      "volume": "16",
      "pages": "358",
      "publicationTitle": "Information-an International Interdisciplinary Journal",
      "DOI": "10.3390/info16050358",
      "issue": "5",
      "journalAbbreviation": "Information",
      "creators": [
        {
          "firstName": "Iqra",
          "lastName": "Mirza",
          "creatorType": "author"
        },
        {
          "firstName": "Ahmad Ali",
          "lastName": "Jafari",
          "creatorType": "author"
        },
        {
          "firstName": "Cihan",
          "lastName": "Ozcinar",
          "creatorType": "author"
        },
        {
          "firstName": "Gholamreza",
          "lastName": "Anbarjafari",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {
        "dc:replaces": [
          "http://zotero.org/users/12031879/items/MX46RKY9"
        ]
      },
      "dateAdded": "2025-11-23T03:44:20Z",
      "dateModified": "2025-11-25T01:06:53Z",
      "uri": "http://zotero.org/users/12031879/items/8PF8T4KB",
      "itemID": 2219,
      "attachments": [],
      "notes": [],
      "citationKey": "mirza2025quantifying",
      "itemKey": "8PF8T4KB",
      "libraryID": 1,
      "select": "zotero://select/library/items/8PF8T4KB"
    },
    {
      "key": "YGPQCAZW",
      "version": 5151,
      "itemType": "conferencePaper",
      "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
      "date": "2021",
      "language": "en",
      "shortTitle": "StereoSet",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://aclanthology.org/2021.acl-long.416",
      "accessDate": "2025-11-25T01:15:20Z",
      "place": "Online",
      "publisher": "Association for Computational Linguistics",
      "pages": "5356-5371",
      "proceedingsTitle": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
      "conferenceName": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
      "DOI": "10.18653/v1/2021.acl-long.416",
      "creators": [
        {
          "firstName": "Moin",
          "lastName": "Nadeem",
          "creatorType": "author"
        },
        {
          "firstName": "Anna",
          "lastName": "Bethke",
          "creatorType": "author"
        },
        {
          "firstName": "Siva",
          "lastName": "Reddy",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-25T01:15:20Z",
      "dateModified": "2025-11-25T01:15:20Z",
      "uri": "http://zotero.org/users/12031879/items/YGPQCAZW",
      "itemID": 2236,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Full Text",
          "url": "https://aclanthology.org/2021.acl-long.416.pdf",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-25T01:15:21Z",
          "dateModified": "2025-11-25T01:15:21Z",
          "uri": "http://zotero.org/users/12031879/items/KIFD22GS",
          "path": "C:\\Users\\lilyc\\Zotero\\storage\\KIFD22GS\\Nadeem et al. - 2021 - StereoSet Measuring stereotypical bias in pretrained language models.pdf",
          "select": "zotero://select/library/items/KIFD22GS"
        }
      ],
      "notes": [],
      "citationKey": "nadeemStereoSetMeasuringStereotypical2021",
      "itemKey": "YGPQCAZW",
      "libraryID": 1,
      "select": "zotero://select/library/items/YGPQCAZW"
    },
    {
      "key": "HYE7XIWB",
      "version": 5151,
      "itemType": "conferencePaper",
      "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
      "date": "2020",
      "language": "en",
      "shortTitle": "CrowS-Pairs",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://www.aclweb.org/anthology/2020.emnlp-main.154",
      "accessDate": "2025-11-25T01:15:31Z",
      "place": "Online",
      "publisher": "Association for Computational Linguistics",
      "pages": "1953-1967",
      "proceedingsTitle": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "conferenceName": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "DOI": "10.18653/v1/2020.emnlp-main.154",
      "creators": [
        {
          "firstName": "Nikita",
          "lastName": "Nangia",
          "creatorType": "author"
        },
        {
          "firstName": "Clara",
          "lastName": "Vania",
          "creatorType": "author"
        },
        {
          "firstName": "Rasika",
          "lastName": "Bhalerao",
          "creatorType": "author"
        },
        {
          "firstName": "Samuel R.",
          "lastName": "Bowman",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-25T01:15:31Z",
      "dateModified": "2025-11-25T01:15:31Z",
      "uri": "http://zotero.org/users/12031879/items/HYE7XIWB",
      "itemID": 2238,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Submitted Version",
          "url": "https://arxiv.org/pdf/2010.00133",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-25T01:15:32Z",
          "dateModified": "2025-11-25T01:15:32Z",
          "uri": "http://zotero.org/users/12031879/items/DV52DSWM",
          "path": "C:\\Users\\lilyc\\Zotero\\storage\\DV52DSWM\\Nangia et al. - 2020 - CrowS-Pairs A Challenge Dataset for Measuring Social Biases in Masked Language Models.pdf",
          "select": "zotero://select/library/items/DV52DSWM"
        }
      ],
      "notes": [],
      "citationKey": "nangiaCrowSPairsChallengeDataset2020",
      "itemKey": "HYE7XIWB",
      "libraryID": 1,
      "select": "zotero://select/library/items/HYE7XIWB"
    },
    {
      "key": "TLLSM59Q",
      "version": 5024,
      "itemType": "journalArticle",
      "title": "Gender stereotypes embedded in natural language are stronger in more economically developed and individualistic countries",
      "date": "2023",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC10662454/",
      "extra": "Citation Key: napp_gender_2023",
      "volume": "2",
      "pages": "pgad355",
      "publicationTitle": "PNAS Nexus",
      "DOI": "10.1093/pnasnexus/pgad355",
      "issue": "11",
      "ISSN": "2752-6542",
      "creators": [
        {
          "firstName": "Clotilde",
          "lastName": "Napp",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T03:44:20Z",
      "dateModified": "2025-11-23T03:44:20Z",
      "uri": "http://zotero.org/users/12031879/items/TLLSM59Q",
      "itemID": 2162,
      "attachments": [],
      "notes": [],
      "citationKey": "napp_gender_2023",
      "itemKey": "TLLSM59Q",
      "libraryID": 1,
      "select": "zotero://select/library/items/TLLSM59Q"
    },
    {
      "key": "FC7SNGAL",
      "version": 5110,
      "itemType": "journalArticle",
      "title": "Biases in Large Language Models: Origins, Inventory, and Discussion",
      "abstractNote": "In this article, we introduce and discuss the pervasive issue of bias in the large language models that are currently at the core of mainstream approaches to Natural Language Processing (NLP). We first introduce data selection bias, that is, the bias caused by the choice of texts that make up a training corpus. Then, we survey the different types of social bias evidenced in the text generated by language models trained on such corpora, ranging from gender to age, from sexual orientation to ethnicity, and from religion to culture. We conclude with directions focused on measuring, reducing, and tackling the aforementioned types of bias.",
      "date": "2023-06-30",
      "language": "en",
      "shortTitle": "Biases in Large Language Models",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://dl.acm.org/doi/10.1145/3597307",
      "accessDate": "2025-11-23T04:20:07Z",
      "volume": "15",
      "pages": "1-21",
      "publicationTitle": "Journal of Data and Information Quality",
      "DOI": "10.1145/3597307",
      "issue": "2",
      "journalAbbreviation": "J. Data and Information Quality",
      "ISSN": "1936-1955, 1936-1963",
      "creators": [
        {
          "firstName": "Roberto",
          "lastName": "Navigli",
          "creatorType": "author"
        },
        {
          "firstName": "Simone",
          "lastName": "Conia",
          "creatorType": "author"
        },
        {
          "firstName": "Björn",
          "lastName": "Ross",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T04:20:07Z",
      "dateModified": "2025-11-23T04:20:07Z",
      "uri": "http://zotero.org/users/12031879/items/FC7SNGAL",
      "itemID": 2228,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Full Text",
          "url": "https://dl.acm.org/doi/pdf/10.1145/3597307",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-23T04:20:07Z",
          "dateModified": "2025-11-23T04:20:07Z",
          "uri": "http://zotero.org/users/12031879/items/4CIWPCW7",
          "path": "C:\\Users\\lilyc\\Zotero\\storage\\4CIWPCW7\\Navigli et al. - 2023 - Biases in Large Language Models Origins, Inventory, and Discussion.pdf",
          "select": "zotero://select/library/items/4CIWPCW7"
        }
      ],
      "notes": [],
      "citationKey": "navigliBiasesLargeLanguage2023",
      "itemKey": "FC7SNGAL",
      "libraryID": 1,
      "select": "zotero://select/library/items/FC7SNGAL"
    },
    {
      "key": "VHNZ35UX",
      "version": 5038,
      "itemType": "conferencePaper",
      "title": "Investigating gender bias in language models using causal mediation analysis",
      "date": "2020",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf",
      "extra": "Citation Key: NEURIPS2020_92650b2e",
      "volume": "33",
      "publisher": "Curran Associates, Inc.",
      "pages": "12388–12401",
      "proceedingsTitle": "Advances in neural information processing systems",
      "creators": [
        {
          "firstName": "Jesse",
          "lastName": "Vig",
          "creatorType": "author"
        },
        {
          "firstName": "Sebastian",
          "lastName": "Gehrmann",
          "creatorType": "author"
        },
        {
          "firstName": "Yonatan",
          "lastName": "Belinkov",
          "creatorType": "author"
        },
        {
          "firstName": "Sharon",
          "lastName": "Qian",
          "creatorType": "author"
        },
        {
          "firstName": "Daniel",
          "lastName": "Nevo",
          "creatorType": "author"
        },
        {
          "firstName": "Yaron",
          "lastName": "Singer",
          "creatorType": "author"
        },
        {
          "firstName": "Stuart",
          "lastName": "Shieber",
          "creatorType": "author"
        },
        {
          "firstName": "H.",
          "lastName": "Larochelle",
          "creatorType": "editor"
        },
        {
          "firstName": "M.",
          "lastName": "Ranzato",
          "creatorType": "editor"
        },
        {
          "firstName": "R.",
          "lastName": "Hadsell",
          "creatorType": "editor"
        },
        {
          "firstName": "M.F.",
          "lastName": "Balcan",
          "creatorType": "editor"
        },
        {
          "firstName": "H.",
          "lastName": "Lin",
          "creatorType": "editor"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T03:48:48Z",
      "dateModified": "2025-11-23T03:48:48Z",
      "uri": "http://zotero.org/users/12031879/items/VHNZ35UX",
      "itemID": 2176,
      "attachments": [],
      "notes": [],
      "citationKey": "NEURIPS2020_92650b2e",
      "itemKey": "VHNZ35UX",
      "libraryID": 1,
      "select": "zotero://select/library/items/VHNZ35UX"
    },
    {
      "key": "7STXMH49",
      "version": 5024,
      "itemType": "journalArticle",
      "title": "Gender bias in video game dialogue",
      "date": "2023",
      "url": "https://royalsocietypublishing.org/doi/10.1098/rsos.221095",
      "extra": "Citation Key: rennick_gender_2023",
      "volume": "10",
      "pages": "221095",
      "publicationTitle": "Royal Society Open Science",
      "DOI": "10.1098/rsos.221095",
      "issue": "5",
      "creators": [
        {
          "firstName": "Stephanie",
          "lastName": "Rennick",
          "creatorType": "author"
        },
        {
          "firstName": "Melanie",
          "lastName": "Clinton",
          "creatorType": "author"
        },
        {
          "firstName": "Elena",
          "lastName": "Ioannidou",
          "creatorType": "author"
        },
        {
          "firstName": "Liana",
          "lastName": "Oh",
          "creatorType": "author"
        },
        {
          "firstName": "Charlotte",
          "lastName": "Clooney",
          "creatorType": "author"
        },
        {
          "firstName": "E.",
          "lastName": "T.",
          "creatorType": "author"
        },
        {
          "firstName": "Edward",
          "lastName": "Healy",
          "creatorType": "author"
        },
        {
          "firstName": "Seán G.",
          "lastName": "Roberts",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T03:44:20Z",
      "dateModified": "2025-11-23T03:44:20Z",
      "uri": "http://zotero.org/users/12031879/items/7STXMH49",
      "itemID": 2164,
      "attachments": [],
      "notes": [],
      "citationKey": "rennick_gender_2023",
      "itemKey": "7STXMH49",
      "libraryID": 1,
      "select": "zotero://select/library/items/7STXMH49"
    },
    {
      "key": "YPNCFST9",
      "version": 5090,
      "itemType": "journalArticle",
      "title": "Evaluating gender bias in large language models in long-term care",
      "date": "2025",
      "extra": "Citation Key: rickman2025longtermcare",
      "volume": "25",
      "pages": "274",
      "publicationTitle": "BMC Medical Informatics and Decision Making",
      "DOI": "10.1186/s12911-025-03118-0",
      "issue": "1",
      "creators": [
        {
          "firstName": "Stephanie",
          "lastName": "Rickman",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T04:17:17Z",
      "dateModified": "2025-11-23T04:17:17Z",
      "uri": "http://zotero.org/users/12031879/items/YPNCFST9",
      "itemID": 2220,
      "attachments": [],
      "notes": [],
      "citationKey": "rickman2025longtermcare",
      "itemKey": "YPNCFST9",
      "libraryID": 1,
      "select": "zotero://select/library/items/YPNCFST9"
    },
    {
      "key": "JSYUKE8P",
      "version": 5151,
      "itemType": "conferencePaper",
      "title": "Gender Bias in Coreference Resolution",
      "date": "2018",
      "language": "en",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "http://aclweb.org/anthology/N18-2002",
      "accessDate": "2025-11-25T01:16:29Z",
      "place": "New Orleans, Louisiana",
      "publisher": "Association for Computational Linguistics",
      "pages": "8-14",
      "proceedingsTitle": "Proceedings of the 2018 Conference of the North American Chapter of           the Association for Computational Linguistics: Human Language           Technologies, Volume 2 (Short Papers)",
      "conferenceName": "Proceedings of the 2018 Conference of the North American Chapter of           the Association for Computational Linguistics: Human Language           Technologies, Volume 2 (Short Papers)",
      "DOI": "10.18653/v1/N18-2002",
      "creators": [
        {
          "firstName": "Rachel",
          "lastName": "Rudinger",
          "creatorType": "author"
        },
        {
          "firstName": "Jason",
          "lastName": "Naradowsky",
          "creatorType": "author"
        },
        {
          "firstName": "Brian",
          "lastName": "Leonard",
          "creatorType": "author"
        },
        {
          "firstName": "Benjamin",
          "lastName": "Van Durme",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-25T01:16:29Z",
      "dateModified": "2025-11-25T01:16:29Z",
      "uri": "http://zotero.org/users/12031879/items/JSYUKE8P",
      "itemID": 2243,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Submitted Version",
          "url": "https://arxiv.org/pdf/1804.09301",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-25T01:16:30Z",
          "dateModified": "2025-11-25T01:16:30Z",
          "uri": "http://zotero.org/users/12031879/items/RZPPSIX2",
          "path": "C:\\Users\\lilyc\\Zotero\\storage\\RZPPSIX2\\Rudinger et al. - 2018 - Gender Bias in Coreference Resolution.pdf",
          "select": "zotero://select/library/items/RZPPSIX2"
        }
      ],
      "notes": [],
      "citationKey": "rudingerGenderBiasCoreference2018",
      "itemKey": "JSYUKE8P",
      "libraryID": 1,
      "select": "zotero://select/library/items/JSYUKE8P"
    },
    {
      "key": "R43ZXL92",
      "version": 5024,
      "itemType": "journalArticle",
      "title": "Gender and media representations: a review of the literature on gender stereotypes, objectification and sexualization",
      "date": "2023",
      "url": "https://www.mdpi.com/1660-4601/20/10/5770",
      "extra": "Citation Key: santoniccolo_gender_2023",
      "volume": "20",
      "pages": "5770",
      "publicationTitle": "International Journal of Environmental Research and Public Health",
      "DOI": "10.3390/ijerph20105770",
      "issue": "10",
      "ISSN": "1660-4601",
      "creators": [
        {
          "firstName": "Fabrizio",
          "lastName": "Santoniccolo",
          "creatorType": "author"
        },
        {
          "firstName": "Tommaso",
          "lastName": "Trombetta",
          "creatorType": "author"
        },
        {
          "firstName": "Maria Noemi",
          "lastName": "Paradiso",
          "creatorType": "author"
        },
        {
          "firstName": "Luca",
          "lastName": "Rollè",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T03:44:20Z",
      "dateModified": "2025-11-23T03:44:20Z",
      "uri": "http://zotero.org/users/12031879/items/R43ZXL92",
      "itemID": 2167,
      "attachments": [],
      "notes": [],
      "citationKey": "santoniccolo_gender_2023",
      "itemKey": "R43ZXL92",
      "libraryID": 1,
      "select": "zotero://select/library/items/R43ZXL92"
    },
    {
      "key": "KVVXN8CV",
      "version": 5110,
      "itemType": "journalArticle",
      "title": "Detecting implicit biases of large language models with Bayesian hypothesis testing",
      "date": "2025-04-11",
      "language": "en",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://www.nature.com/articles/s41598-025-95825-x",
      "accessDate": "2025-11-23T04:21:28Z",
      "volume": "15",
      "pages": "12415",
      "publicationTitle": "Scientific Reports",
      "DOI": "10.1038/s41598-025-95825-x",
      "issue": "1",
      "journalAbbreviation": "Sci Rep",
      "ISSN": "2045-2322",
      "creators": [
        {
          "firstName": "Shijing",
          "lastName": "Si",
          "creatorType": "author"
        },
        {
          "firstName": "Xiaoming",
          "lastName": "Jiang",
          "creatorType": "author"
        },
        {
          "firstName": "Qinliang",
          "lastName": "Su",
          "creatorType": "author"
        },
        {
          "firstName": "Lawrence",
          "lastName": "Carin",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T04:21:28Z",
      "dateModified": "2025-11-23T04:21:28Z",
      "uri": "http://zotero.org/users/12031879/items/KVVXN8CV",
      "itemID": 2232,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Full Text PDF",
          "url": "https://www.nature.com/articles/s41598-025-95825-x.pdf",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-23T04:21:32Z",
          "dateModified": "2025-11-23T04:21:32Z",
          "uri": "http://zotero.org/users/12031879/items/FPNPBH7A",
          "path": "C:\\Users\\lilyc\\Zotero\\storage\\FPNPBH7A\\Si et al. - 2025 - Detecting implicit biases of large language models with Bayesian hypothesis testing.pdf",
          "select": "zotero://select/library/items/FPNPBH7A"
        }
      ],
      "notes": [],
      "citationKey": "siDetectingImplicitBiases2025",
      "itemKey": "KVVXN8CV",
      "libraryID": 1,
      "select": "zotero://select/library/items/KVVXN8CV"
    },
    {
      "key": "L9JEACFP",
      "version": 5044,
      "itemType": "preprint",
      "title": "A Survey on Gender Bias in Natural Language Processing",
      "abstractNote": "Language can be used as a means of reproducing and enforcing harmful stereotypes and biases and has been analysed as such in numerous research. In this paper, we present a survey of 304 papers on gender bias in natural language processing. We analyse definitions of gender and its categories within social sciences and connect them to formal definitions of gender bias in NLP research. We survey lexica and datasets applied in research on gender bias and then compare and contrast approaches to detecting and mitigating gender bias. We find that research on gender bias suffers from four core limitations. 1) Most research treats gender as a binary variable neglecting its fluidity and continuity. 2) Most of the work has been conducted in monolingual setups for English or other high-resource languages. 3) Despite a myriad of papers on gender bias in NLP methods, we find that most of the newly developed algorithms do not test their models for bias and disregard possible ethical considerations of their work. 4) Finally, methodologies developed in this line of research are fundamentally flawed covering very limited definitions of gender bias and lacking evaluation baselines and pipelines. We suggest recommendations towards overcoming these limitations as a guide for future research.",
      "date": "2021-12-28",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2112.14168",
      "accessDate": "2025-11-23T04:00:39Z",
      "extra": "arXiv:2112.14168 [cs]",
      "DOI": "10.48550/arXiv.2112.14168",
      "repository": "arXiv",
      "archiveID": "arXiv:2112.14168",
      "creators": [
        {
          "firstName": "Karolina",
          "lastName": "Stanczak",
          "creatorType": "author"
        },
        {
          "firstName": "Isabelle",
          "lastName": "Augenstein",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Computation and Language",
          "type": 1
        },
        {
          "tag": "Computer Science - Computers and Society",
          "type": 1
        }
      ],
      "relations": {},
      "dateAdded": "2025-11-23T04:00:39Z",
      "dateModified": "2025-11-23T04:00:39Z",
      "uri": "http://zotero.org/users/12031879/items/L9JEACFP",
      "itemID": 2184,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Preprint PDF",
          "url": "http://arxiv.org/pdf/2112.14168v1",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-23T04:00:40Z",
          "dateModified": "2025-11-23T04:00:40Z",
          "uri": "http://zotero.org/users/12031879/items/3KBSWC78",
          "path": "C:\\Users\\lilyc\\Zotero\\storage\\3KBSWC78\\Stanczak and Augenstein - 2021 - A Survey on Gender Bias in Natural Language Processing.pdf",
          "select": "zotero://select/library/items/3KBSWC78"
        },
        {
          "itemType": "attachment",
          "title": "Snapshot",
          "url": "https://arxiv.org/abs/2112.14168",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-23T04:00:44Z",
          "dateModified": "2025-11-23T04:00:44Z",
          "uri": "http://zotero.org/users/12031879/items/SFTE9HF3",
          "path": "C:\\Users\\lilyc\\Zotero\\storage\\SFTE9HF3\\2112.html",
          "select": "zotero://select/library/items/SFTE9HF3"
        }
      ],
      "notes": [],
      "citationKey": "stanczakSurveyGenderBias2021",
      "itemKey": "L9JEACFP",
      "libraryID": 1,
      "select": "zotero://select/library/items/L9JEACFP"
    },
    {
      "key": "7DNQVH5E",
      "version": 5110,
      "itemType": "preprint",
      "title": "Bias in Large Language Models Across Clinical Applications: A Systematic Review",
      "abstractNote": "Background: Large language models (LLMs) are rapidly being integrated into healthcare, promising to enhance various clinical tasks. However, concerns exist regarding their potential for bias, which could compromise patient care and exacerbate health inequities. This systematic review investigates the prevalence, sources, manifestations, and clinical implications of bias in LLMs. Methods: We conducted a systematic search of PubMed, OVID, and EMBASE from database inception through 2025, for studies evaluating bias in LLMs applied to clinical tasks. We extracted data on LLM type, bias source, bias manifestation, affected attributes, clinical task, evaluation methods, and outcomes. Risk of bias was assessed using a modified ROBINS-I tool. Results: Thirty-eight studies met inclusion criteria, revealing pervasive bias across various LLMs and clinical applications. Both data-related bias (from biased training data) and model-related bias (from model training) were significant contributors. Biases manifested as: allocative harm (e.g., differential treatment recommendations); representational harm (e.g., stereotypical associations, biased image generation); and performance disparities (e.g., variable output quality). These biases affected multiple attributes, most frequently race/ethnicity and gender, but also age, disability, and language. Conclusions: Bias in clinical LLMs is a pervasive and systemic issue, with a potential to lead to misdiagnosis and inappropriate treatment, particularly for marginalized patient populations. Rigorous evaluation of the model is crucial. Furthermore, the development and implementation of effective mitigation strategies, coupled with continuous monitoring in real-world clinical settings, are essential to ensure the safe, equitable, and trustworthy deployment of LLMs in healthcare.",
      "date": "2025-04-03",
      "shortTitle": "Bias in Large Language Models Across Clinical Applications",
      "libraryCatalog": "arXiv.org",
      "url": "http://arxiv.org/abs/2504.02917",
      "accessDate": "2025-11-23T04:19:32Z",
      "extra": "arXiv:2504.02917 [cs]",
      "DOI": "10.48550/arXiv.2504.02917",
      "repository": "arXiv",
      "archiveID": "arXiv:2504.02917",
      "creators": [
        {
          "firstName": "Thanathip",
          "lastName": "Suenghataiphorn",
          "creatorType": "author"
        },
        {
          "firstName": "Narisara",
          "lastName": "Tribuddharat",
          "creatorType": "author"
        },
        {
          "firstName": "Pojsakorn",
          "lastName": "Danpanichkul",
          "creatorType": "author"
        },
        {
          "firstName": "Narathorn",
          "lastName": "Kulthamrongsri",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computer Science - Computation and Language",
          "type": 1
        },
        {
          "tag": "Computer Science - Artificial Intelligence",
          "type": 1
        }
      ],
      "relations": {},
      "dateAdded": "2025-11-23T04:19:32Z",
      "dateModified": "2025-11-23T04:19:32Z",
      "uri": "http://zotero.org/users/12031879/items/7DNQVH5E",
      "itemID": 2225,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Preprint PDF",
          "url": "http://arxiv.org/pdf/2504.02917v1",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-23T04:19:32Z",
          "dateModified": "2025-11-23T04:19:32Z",
          "uri": "http://zotero.org/users/12031879/items/PUKYW4C9",
          "path": "C:\\Users\\lilyc\\Zotero\\storage\\PUKYW4C9\\Suenghataiphorn et al. - 2025 - Bias in Large Language Models Across Clinical Applications A Systematic Review.pdf",
          "select": "zotero://select/library/items/PUKYW4C9"
        },
        {
          "itemType": "attachment",
          "title": "Snapshot",
          "url": "https://arxiv.org/abs/2504.02917",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-23T04:19:37Z",
          "dateModified": "2025-11-23T04:19:37Z",
          "uri": "http://zotero.org/users/12031879/items/HZ3EQS8Y",
          "path": "C:\\Users\\lilyc\\Zotero\\storage\\HZ3EQS8Y\\2504.html",
          "select": "zotero://select/library/items/HZ3EQS8Y"
        }
      ],
      "notes": [],
      "citationKey": "suenghataiphornBiasLargeLanguage2025",
      "itemKey": "7DNQVH5E",
      "libraryID": 1,
      "select": "zotero://select/library/items/7DNQVH5E"
    },
    {
      "key": "LXTDAIRI",
      "version": 5158,
      "itemType": "preprint",
      "title": "She Elicits Requirements and He Tests: Software Engineering Gender Bias in Large Language Models",
      "abstractNote": "Implicit gender bias in software development is a well-documented issue, such as the association of technical roles with men. To address this bias, it is important to understand it in more detail. This study uses data mining techniques to investigate the extent to which 56 tasks related to software development, such as assigning GitHub issues and testing, are affected by implicit gender bias embedded in large language models. We systematically translated each task from English into a genderless language and back, and investigated the pronouns associated with each task. Based on translating each task 100 times in different permutations, we identify a significant disparity in the gendered pronoun associations with different tasks. Specifically, requirements elicitation was associated with the pronoun \"he\" in only 6% of cases, while testing was associated with \"he\" in 100% of cases. Additionally, tasks related to helping others had a 91% association with \"he\" while the same association for tasks related to asking coworkers was only 52%. These findings reveal a clear pattern of gender bias related to software development tasks and have important implications for addressing this issue both in the training of large language models and in broader society.",
      "date": "2023",
      "shortTitle": "She Elicits Requirements and He Tests",
      "libraryCatalog": "DOI.org (Datacite)",
      "url": "https://arxiv.org/abs/2303.10131",
      "accessDate": "2025-11-26T23:22:18Z",
      "rights": "Creative Commons Zero v1.0 Universal",
      "extra": "Version Number: 1",
      "DOI": "10.48550/ARXIV.2303.10131",
      "repository": "arXiv",
      "creators": [
        {
          "firstName": "Christoph",
          "lastName": "Treude",
          "creatorType": "author"
        },
        {
          "firstName": "Hideaki",
          "lastName": "Hata",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Artificial Intelligence (cs.AI)",
          "type": 1
        },
        {
          "tag": "Computers and Society (cs.CY)",
          "type": 1
        },
        {
          "tag": "FOS: Computer and information sciences",
          "type": 1
        },
        {
          "tag": "Machine Learning (cs.LG)",
          "type": 1
        },
        {
          "tag": "Software Engineering (cs.SE)",
          "type": 1
        }
      ],
      "relations": {},
      "dateAdded": "2025-11-26T23:22:18Z",
      "dateModified": "2025-11-26T23:22:18Z",
      "uri": "http://zotero.org/users/12031879/items/LXTDAIRI",
      "itemID": 2260,
      "attachments": [],
      "notes": [
        {
          "key": "WBY43N5R",
          "version": 5158,
          "itemType": "note",
          "parentItem": "LXTDAIRI",
          "note": "<h2>Other</h2>\n6 pages, MSR 2023",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-26T23:22:18Z",
          "dateModified": "2025-11-26T23:22:18Z",
          "uri": "http://zotero.org/users/12031879/items/WBY43N5R"
        }
      ],
      "citationKey": "treudeSheElicitsRequirements2023",
      "itemKey": "LXTDAIRI",
      "libraryID": 1,
      "select": "zotero://select/library/items/LXTDAIRI"
    },
    {
      "key": "B6S72K7Q",
      "version": 5152,
      "itemType": "preprint",
      "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
      "abstractNote": "Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always \"(A)\"--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.",
      "date": "2023",
      "shortTitle": "Language Models Don't Always Say What They Think",
      "libraryCatalog": "DOI.org (Datacite)",
      "url": "https://arxiv.org/abs/2305.04388",
      "accessDate": "2025-11-25T01:19:43Z",
      "rights": "Creative Commons Attribution 4.0 International",
      "extra": "Version Number: 2",
      "DOI": "10.48550/ARXIV.2305.04388",
      "repository": "arXiv",
      "creators": [
        {
          "firstName": "Miles",
          "lastName": "Turpin",
          "creatorType": "author"
        },
        {
          "firstName": "Julian",
          "lastName": "Michael",
          "creatorType": "author"
        },
        {
          "firstName": "Ethan",
          "lastName": "Perez",
          "creatorType": "author"
        },
        {
          "firstName": "Samuel R.",
          "lastName": "Bowman",
          "creatorType": "author"
        }
      ],
      "tags": [
        {
          "tag": "Computation and Language (cs.CL)",
          "type": 1
        },
        {
          "tag": "FOS: Computer and information sciences",
          "type": 1
        },
        {
          "tag": "Artificial Intelligence (cs.AI)",
          "type": 1
        }
      ],
      "relations": {},
      "dateAdded": "2025-11-25T01:19:43Z",
      "dateModified": "2025-11-25T01:19:43Z",
      "uri": "http://zotero.org/users/12031879/items/B6S72K7Q",
      "itemID": 2256,
      "attachments": [],
      "notes": [
        {
          "key": "TW3ZVXS2",
          "version": 5150,
          "itemType": "note",
          "parentItem": "B6S72K7Q",
          "note": "<h2>Other</h2>\nNeurIPS 2023",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-25T01:19:43Z",
          "dateModified": "2025-11-25T01:19:43Z",
          "uri": "http://zotero.org/users/12031879/items/TW3ZVXS2"
        }
      ],
      "citationKey": "turpinLanguageModelsDont2023",
      "itemKey": "B6S72K7Q",
      "libraryID": 1,
      "select": "zotero://select/library/items/B6S72K7Q"
    },
    {
      "key": "37SZ2XBL",
      "version": 5152,
      "itemType": "journalArticle",
      "title": "Does ChatGPT show gender bias in behavior detection?",
      "date": "2024-12-21",
      "language": "en",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://www.nature.com/articles/s41599-024-04219-3",
      "accessDate": "2025-11-25T01:19:13Z",
      "volume": "11",
      "pages": "1706",
      "publicationTitle": "Humanities and Social Sciences Communications",
      "DOI": "10.1057/s41599-024-04219-3",
      "issue": "1",
      "journalAbbreviation": "Humanit Soc Sci Commun",
      "ISSN": "2662-9992",
      "creators": [
        {
          "firstName": "Ji",
          "lastName": "Wu",
          "creatorType": "author"
        },
        {
          "firstName": "Yaokang",
          "lastName": "Song",
          "creatorType": "author"
        },
        {
          "firstName": "Doris Chenguang",
          "lastName": "Wu",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-25T01:19:13Z",
      "dateModified": "2025-11-25T01:19:13Z",
      "uri": "http://zotero.org/users/12031879/items/37SZ2XBL",
      "itemID": 2253,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Full Text PDF",
          "url": "https://www.nature.com/articles/s41599-024-04219-3.pdf",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-25T01:19:17Z",
          "dateModified": "2025-11-25T01:19:17Z",
          "uri": "http://zotero.org/users/12031879/items/A8GZSIQ3",
          "path": "C:\\Users\\lilyc\\Zotero\\storage\\A8GZSIQ3\\Wu et al. - 2024 - Does ChatGPT show gender bias in behavior detection.pdf",
          "select": "zotero://select/library/items/A8GZSIQ3"
        }
      ],
      "notes": [],
      "citationKey": "wuDoesChatGPTShow2024",
      "itemKey": "37SZ2XBL",
      "libraryID": 1,
      "select": "zotero://select/library/items/37SZ2XBL"
    },
    {
      "key": "72V8HUX2",
      "version": 5090,
      "itemType": "document",
      "title": "Auto-search and refinement: An automated framework for gender bias mitigation in large language models",
      "date": "2025",
      "extra": "Citation Key: xu2025autosearch\narXiv: 2502.11559 [cs.CL]\nDOI: 10.48550/arXiv.2502.11559",
      "creators": [
        {
          "firstName": "Yiqing",
          "lastName": "Xu",
          "creatorType": "author"
        },
        {
          "firstName": "Chen",
          "lastName": "Fu",
          "creatorType": "author"
        },
        {
          "firstName": "Li",
          "lastName": "Xiong",
          "creatorType": "author"
        },
        {
          "firstName": "Shan",
          "lastName": "Yang",
          "creatorType": "author"
        },
        {
          "firstName": "Weiqing",
          "lastName": "Wang",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T04:17:17Z",
      "dateModified": "2025-11-23T04:17:17Z",
      "uri": "http://zotero.org/users/12031879/items/72V8HUX2",
      "itemID": 2221,
      "attachments": [],
      "notes": [],
      "citationKey": "xu2025autosearch",
      "itemKey": "72V8HUX2",
      "libraryID": 1,
      "select": "zotero://select/library/items/72V8HUX2"
    },
    {
      "key": "2QJ9VIXE",
      "version": 5152,
      "itemType": "conferencePaper",
      "title": "Blind Men and the Elephant: Diverse Perspectives on Gender Stereotypes in Benchmark Datasets",
      "date": "2025",
      "language": "en",
      "shortTitle": "Blind Men and the Elephant",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "https://aclanthology.org/2025.emnlp-main.1162",
      "accessDate": "2025-11-25T01:19:28Z",
      "place": "Suzhou, China",
      "publisher": "Association for Computational Linguistics",
      "pages": "22838-22851",
      "proceedingsTitle": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "conferenceName": "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
      "DOI": "10.18653/v1/2025.emnlp-main.1162",
      "creators": [
        {
          "firstName": "Mahdi",
          "lastName": "Zakizadeh",
          "creatorType": "author"
        },
        {
          "firstName": "Mohammad Taher",
          "lastName": "Pilehvar",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-25T01:19:28Z",
      "dateModified": "2025-11-25T01:19:28Z",
      "uri": "http://zotero.org/users/12031879/items/2QJ9VIXE",
      "itemID": 2255,
      "attachments": [],
      "notes": [],
      "citationKey": "zakizadehBlindMenElephant2025",
      "itemKey": "2QJ9VIXE",
      "libraryID": 1,
      "select": "zotero://select/library/items/2QJ9VIXE"
    },
    {
      "key": "4QZE7GEW",
      "version": 5090,
      "itemType": "document",
      "title": "Gender bias in large language models across multiple languages",
      "date": "2024",
      "extra": "Citation Key: zhao2024multilingualbias\narXiv: 2403.00277 [cs.CL]\nDOI: 10.48550/arXiv.2403.00277",
      "creators": [
        {
          "firstName": "Jinlong",
          "lastName": "Zhao",
          "creatorType": "author"
        },
        {
          "firstName": "Yining",
          "lastName": "Ding",
          "creatorType": "author"
        },
        {
          "firstName": "Changqian",
          "lastName": "Jia",
          "creatorType": "author"
        },
        {
          "firstName": "Yun",
          "lastName": "Wang",
          "creatorType": "author"
        },
        {
          "firstName": "Zhendong",
          "lastName": "Qian",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T04:17:17Z",
      "dateModified": "2025-11-23T04:17:17Z",
      "uri": "http://zotero.org/users/12031879/items/4QZE7GEW",
      "itemID": 2222,
      "attachments": [],
      "notes": [],
      "citationKey": "zhao2024multilingualbias",
      "itemKey": "4QZE7GEW",
      "libraryID": 1,
      "select": "zotero://select/library/items/4QZE7GEW"
    },
    {
      "key": "I248YNUB",
      "version": 5151,
      "itemType": "conferencePaper",
      "title": "Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods",
      "date": "2018",
      "language": "en",
      "shortTitle": "Gender Bias in Coreference Resolution",
      "libraryCatalog": "DOI.org (Crossref)",
      "url": "http://aclweb.org/anthology/N18-2003",
      "accessDate": "2025-11-25T01:16:39Z",
      "place": "New Orleans, Louisiana",
      "publisher": "Association for Computational Linguistics",
      "pages": "15-20",
      "proceedingsTitle": "Proceedings of the 2018 Conference of the North American Chapter of           the Association for Computational Linguistics: Human Language           Technologies, Volume 2 (Short Papers)",
      "conferenceName": "Proceedings of the 2018 Conference of the North American Chapter of           the Association for Computational Linguistics: Human Language           Technologies, Volume 2 (Short Papers)",
      "DOI": "10.18653/v1/N18-2003",
      "creators": [
        {
          "firstName": "Jieyu",
          "lastName": "Zhao",
          "creatorType": "author"
        },
        {
          "firstName": "Tianlu",
          "lastName": "Wang",
          "creatorType": "author"
        },
        {
          "firstName": "Mark",
          "lastName": "Yatskar",
          "creatorType": "author"
        },
        {
          "firstName": "Vicente",
          "lastName": "Ordonez",
          "creatorType": "author"
        },
        {
          "firstName": "Kai-Wei",
          "lastName": "Chang",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-25T01:16:39Z",
      "dateModified": "2025-11-25T01:16:39Z",
      "uri": "http://zotero.org/users/12031879/items/I248YNUB",
      "itemID": 2245,
      "attachments": [
        {
          "itemType": "attachment",
          "title": "Submitted Version",
          "url": "https://arxiv.org/pdf/1804.06876",
          "tags": [],
          "relations": {},
          "dateAdded": "2025-11-25T01:16:40Z",
          "dateModified": "2025-11-25T01:16:40Z",
          "uri": "http://zotero.org/users/12031879/items/ZL69C477",
          "path": "C:\\Users\\lilyc\\Zotero\\storage\\ZL69C477\\Zhao et al. - 2018 - Gender Bias in Coreference Resolution Evaluation and Debiasing Methods.pdf",
          "select": "zotero://select/library/items/ZL69C477"
        }
      ],
      "notes": [],
      "citationKey": "zhaoGenderBiasCoreference2018",
      "itemKey": "I248YNUB",
      "libraryID": 1,
      "select": "zotero://select/library/items/I248YNUB"
    },
    {
      "key": "HENUGYG2",
      "version": 5090,
      "itemType": "journalArticle",
      "title": "Evaluating and mitigating gender bias in generative large language models",
      "date": "2024",
      "extra": "Citation Key: zhou2024generativellm",
      "volume": "19",
      "publicationTitle": "International Journal of Computers Communications & Control",
      "DOI": "10.15837/ijccc.2024.6.6853",
      "issue": "6",
      "creators": [
        {
          "firstName": "Hui",
          "lastName": "Zhou",
          "creatorType": "author"
        },
        {
          "firstName": "Diana",
          "lastName": "Inkpen",
          "creatorType": "author"
        },
        {
          "firstName": "Burak",
          "lastName": "Kantarci",
          "creatorType": "author"
        }
      ],
      "tags": [],
      "relations": {},
      "dateAdded": "2025-11-23T04:17:17Z",
      "dateModified": "2025-11-23T04:17:17Z",
      "uri": "http://zotero.org/users/12031879/items/HENUGYG2",
      "itemID": 2223,
      "attachments": [],
      "notes": [],
      "citationKey": "zhou2024generativellm",
      "itemKey": "HENUGYG2",
      "libraryID": 1,
      "select": "zotero://select/library/items/HENUGYG2"
    }
  ]
}